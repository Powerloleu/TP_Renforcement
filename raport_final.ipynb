{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPAR - Rapport du rendu II\n",
    "**01/04/24**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Ce notebook présente l'explication, la démonstration et les résultats des outils développés dans le cadre du cours de MPAR. Les résultats de la première partie du rendu, discutés en cours, ne seront pas abordés ici mais seront uniquement utilisés comme référence.\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "1. Comprendre les concepts de **Model Checking** et de **SMC (Statistical Model Checking)** pour les **chaînes de Markov**.\n",
    "2. Étendre ces concepts aux **MDP (Markov Decision Process)**.\n",
    "3. Utiliser le **RL (Reinforcement Learning)** pour améliorer le model checking statistique.\n",
    "\n",
    "## Contenu\n",
    "1. **Changements dans le parser**\n",
    "2. **Chapitre 2, Vérification Probabiliste**\n",
    "3. **Chapitre 3, Modélisation Probabiliste et Apprentissage par Renforcement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Changements dans le parser\n",
    "\n",
    "Pour pouvoir utiliser des modèles .mdp avec des recompenses, il a fallut modifier notre parser. Pour ceci, les anciens modèles .mdp, sans récompenses, doivent continuer a fonctionnner sans altération.\n",
    "\n",
    "De cette façon, nous avons inclu la ligne optionnelle `Rewards` avec des entrées dans le format `s:r` avec s un état et r une récompense entière. L'exemple `states_with_rewards` montre ceci.\n",
    "\n",
    "Ceci a été fait en ajoutant l'entrée (defrewards) optionnele (?)  a la définition du programme, ayant `program: defstates defrewards? defactions transitions EOF;` et ayant la définition `defewrards : REWARDS ID ':' INT (',' ID ':' INT)* ';';`.\n",
    "\n",
    "Après, nous avons ajouté l'attribut `rewards` à la classe `gramPrintListener`, défini a travers les fonctions `enterDefrewards` et `update_rewards`.\n",
    "\n",
    "De cette façon, comme dans l'exemple vu en cours, il suffit d'importer la fonction `run` de l'archive `mdp.py` pour lire un fichier et creer un objet de la classe `gramPrintListener` qui contiendrat, en plus des attributs vus dans la prémière démostration, les attributs de récompenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import run as run_mdp # Fonction qui crée l'objet a être lu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple sans récompenses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['I', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6']\n",
      "Initialy declared actions: ['a']\n",
      "Transition from I with no action and targets ['T1', 'T2'] with weights [1, 1]\n",
      "Transition from T1 with no action and targets ['T3', 'T4'] with weights [1, 1]\n",
      "Transition from T2 with no action and targets ['T5', 'T6'] with weights [1, 1]\n",
      "Transition from T3 with no action and targets ['S1', 'T1'] with weights [1, 1]\n",
      "Transition from T4 with no action and targets ['S2', 'S3'] with weights [1, 1]\n",
      "Transition from T5 with no action and targets ['S4', 'S5'] with weights [1, 1]\n",
      "Transition from T6 with no action and targets ['S6', 'T2'] with weights [1, 1]\n",
      "Transition from S1 with no action and targets ['S1'] with weights [1]\n",
      "Transition from S2 with no action and targets ['S2'] with weights [1]\n",
      "Transition from S3 with no action and targets ['S3'] with weights [1]\n",
      "Transition from S4 with no action and targets ['S4'] with weights [1]\n",
      "Transition from S5 with no action and targets ['S5'] with weights [1]\n",
      "Transition from S6 with no action and targets ['S6'] with weights [1]\n",
      "\n",
      " ------- transactions df -------\n",
      "  Origin Action    I   T1   T2   T3   T4   T5   T6   S1   S2   S3   S4   S5  \\\n",
      "0      I     NA  NaN    1    1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "1     T1     NA  NaN  NaN  NaN    1    1  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "2     T2     NA  NaN  NaN  NaN  NaN  NaN    1    1  NaN  NaN  NaN  NaN  NaN   \n",
      "3     T3     NA  NaN    1  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN  NaN  NaN   \n",
      "4     T4     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1    1  NaN  NaN   \n",
      "5     T5     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1    1   \n",
      "6     T6     NA  NaN  NaN    1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "7     S1     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN  NaN  NaN   \n",
      "8     S2     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN  NaN   \n",
      "9     S3     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN   \n",
      "\n",
      "    S6  \n",
      "0  NaN  \n",
      "1  NaN  \n",
      "2  NaN  \n",
      "3  NaN  \n",
      "4  NaN  \n",
      "5  NaN  \n",
      "6    1  \n",
      "7  NaN  \n",
      "8  NaN  \n",
      "9  NaN  \n",
      "\n",
      " ------- transactions_prob df -------\n",
      "  Origin Action    I   T1   T2   T3   T4   T5   T6   S1   S2   S3   S4   S5  \\\n",
      "0      I     NA  0.0  0.5  0.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "1     T1     NA  0.0  0.0  0.0  0.5  0.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2     T2     NA  0.0  0.0  0.0  0.0  0.0  0.5  0.5  0.0  0.0  0.0  0.0  0.0   \n",
      "3     T3     NA  0.0  0.5  0.0  0.0  0.0  0.0  0.0  0.5  0.0  0.0  0.0  0.0   \n",
      "4     T4     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.5  0.5  0.0  0.0   \n",
      "5     T5     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.5  0.5   \n",
      "6     T6     NA  0.0  0.0  0.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "7     S1     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
      "8     S2     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
      "9     S3     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
      "\n",
      "    S6  \n",
      "0  0.0  \n",
      "1  0.0  \n",
      "2  0.0  \n",
      "3  0.0  \n",
      "4  0.0  \n",
      "5  0.0  \n",
      "6  0.5  \n",
      "7  0.0  \n",
      "8  0.0  \n",
      "9  0.0   \n",
      "\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - State I reward wasn't assigned, using zero as reward\n",
      "( 1 ) - State T1 reward wasn't assigned, using zero as reward\n",
      "( 2 ) - State T2 reward wasn't assigned, using zero as reward\n",
      "( 3 ) - State T3 reward wasn't assigned, using zero as reward\n",
      "( 4 ) - State T4 reward wasn't assigned, using zero as reward\n",
      "( 5 ) - State T5 reward wasn't assigned, using zero as reward\n",
      "( 6 ) - State T6 reward wasn't assigned, using zero as reward\n",
      "( 7 ) - State S1 reward wasn't assigned, using zero as reward\n",
      "( 8 ) - State S2 reward wasn't assigned, using zero as reward\n",
      "( 9 ) - State S3 reward wasn't assigned, using zero as reward\n",
      "( 10 ) - State S4 reward wasn't assigned, using zero as reward\n",
      "( 11 ) - State S5 reward wasn't assigned, using zero as reward\n",
      "( 12 ) - State S6 reward wasn't assigned, using zero as reward\n"
     ]
    }
   ],
   "source": [
    "simu_mc = run_mdp(path = \"prof_examples//simu-mc.mdp\", return_printer=True, print_transactions=True) # Exemple sans récompenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple avec récompenses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['S0', 'S1', 'S2']\n",
      "Initialy declared actions: ['a', 'b', 'c']\n",
      "Transition from S0 with no action and targets ['S1', 'S2'] with weights [5, 5]\n",
      "Transition from S1 with action b and targets ['S1', 'S0'] with weights [2, 8]\n",
      "Transition from S1 with action a and targets ['S2', 'S0', 'S1', 'S3'] with weights [1, 3, 6, 2]\n",
      "Transition from S2 with action c and targets ['S0', 'S1', 'S3'] with weights [5, 5, 10]\n",
      "Transition from S2 with action d and targets ['S0', 'S3'] with weights [5, 7]\n",
      "Transition from S3 with action e and targets ['S1', 'S2'] with weights [2, 2]\n",
      "\n",
      " ------- transactions df -------\n",
      "  Origin Action   S0   S1   S2    S3\n",
      "0     S0     NA  NaN    5    5   NaN\n",
      "1     S1      b    8    2  NaN   NaN\n",
      "2     S1      a    3    6    1   2.0\n",
      "3     S2      c    5    5  NaN  10.0\n",
      "4     S2      d    5  NaN  NaN   7.0\n",
      "5     S3      e  NaN    2    2   NaN\n",
      "\n",
      " ------- transactions_prob df -------\n",
      "  Origin Action        S0    S1        S2        S3\n",
      "0     S0     NA  0.000000  0.50  0.500000  0.000000\n",
      "1     S1      b  0.800000  0.20  0.000000  0.000000\n",
      "2     S1      a  0.250000  0.50  0.083333  0.166667\n",
      "3     S2      c  0.250000  0.25  0.000000  0.500000\n",
      "4     S2      d  0.416667  0.00  0.000000  0.583333\n",
      "5     S3      e  0.000000  0.50  0.500000  0.000000 \n",
      "\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - Undeclared state S3 targeted in transition: S1 with action a, declared automaticaly\n",
      "( 1 ) - Undeclared action in transition: S2 with action d, declared automaticaly\n",
      "( 2 ) - Undeclared action in transition: S3 with action e, declared automaticaly\n",
      "( 3 ) - State S3 reward wasn't assigned, using zero as reward\n"
     ]
    }
   ],
   "source": [
    "states_with_rewards = run_mdp(path = \"mdp_examples//states_with_rewards.mdp\", return_printer=True, print_transactions=True) # Exemple avec récompenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme montré dans les exemples précedents, à chaque état nous avons un reward attribué. \n",
    "\n",
    "Si la récompense n'a pas été définie, notre parser attribut une valeur de zero. \n",
    "\n",
    "Il est un dictionnaire et peut être appelé facilement par l'attribut `rewards` comme suit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states_with_reards.mdp {'S0': 1, 'S1': 10, 'S2': 15, 'S3': 0}\n",
      "simu_mc.mdp {'I': 0, 'T1': 0, 'T2': 0, 'T3': 0, 'T4': 0, 'T5': 0, 'T6': 0, 'S1': 0, 'S2': 0, 'S3': 0, 'S4': 0, 'S5': 0, 'S6': 0}\n"
     ]
    }
   ],
   "source": [
    "print('states_with_reards.mdp', states_with_rewards.rewards)\n",
    "print('simu_mc.mdp', simu_mc.rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vérification Probabiliste (Chapitre 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 12: \n",
    "Identification des ensembles $S_0$, $S_1$ et $S_?$ pour une propriété $P(\\diamond s)$ (fatalment $s$, où $s$ est un état)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBS: Está errado, mesmo corrigindo para MDPs, também dá erro se um estado passar por outro com prob 1. A ideia do algoritmo em si está errada pq só pega transições diretas, o que não faz sentido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working for both MDPs and MCs.\n",
    "def find_states(printer, target_state):\n",
    "    \"\"\"\n",
    "    Identifies states leading to a target state with certainty (S_sure), states that may lead to the target state (S_may),\n",
    "    and states that can never reach the target state (S_never) in a Markov chain represented by a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - printer - our object of class gramPrintListener\n",
    "    - target_state (str): The state of interest to trace back from.\n",
    "\n",
    "    Returns:\n",
    "    - tuple of lists: A tuple containing three lists representing states that are sure to reach the target (S_sure),\n",
    "                      states that may reach the target (S_may), and states that can never reach the target (S_never).\n",
    "    \"\"\"\n",
    "    df = printer.transactions_prob\n",
    "    \n",
    "    # Initialize sets for S_sure, S_may, and S_never. We begin with all as never\n",
    "    S_sure = set()          \n",
    "    S_may = set()           \n",
    "    S_never = set(printer.declared_states)   \n",
    "\n",
    "    # The target state of in S_sure\n",
    "    S_sure.add(target_state)\n",
    "    S_never.remove(target_state)\n",
    "    \n",
    "    # States visited or to visit\n",
    "    visited = set()\n",
    "    to_visit = [target_state]\n",
    "\n",
    "    # Tant que il faut encore visiter un état\n",
    "    while to_visit:\n",
    "        current_state = to_visit.pop() # Choisir l'un de ceux a visiter\n",
    "        visited.add(current_state)     # Il a été visité\n",
    "        for origin in printer.declared_states:          # Pour chaque origine:\n",
    "            if len(df.loc[df['Origin'] == origin]) == 1: # If there's only one possible action or no actions:\n",
    "                if df.loc[df['Origin'] == origin, current_state].values[0] > 0: # Si on peut arriver depuis ce état au état analysée\n",
    "                    if df.loc[df['Origin'] == origin, current_state].values[0] < 1.0: # Si cette probabilitée d'arriver n'est pas sure\n",
    "                        S_may.add(origin)    # We may arrive\n",
    "                        if origin in S_never:\n",
    "                            S_never.remove(origin) # Et on l'envlève des never\n",
    "                        if origin not in visited:  # Et on doit le visiter\n",
    "                            to_visit.append(origin)\n",
    "                    else:                   # Si la probabilité est sure\n",
    "                        S_sure.add(origin)  # Nous avons un S_sure\n",
    "                        if origin in S_never:\n",
    "                            S_never.remove(origin) # Donc pas never\n",
    "            else: # Si il y a plusieurs actions possibles\n",
    "                if df.loc[df['Origin'] == origin, current_state].values.sum() > 0: # Et il y a au moins une action avec une probabilité différente de zero d'arriver\n",
    "                    if 1.0 not in df.loc[df['Origin'] == origin, current_state].values: # Si aucune est de 1\n",
    "                        S_may.add(origin) # May arrive\n",
    "                        if origin in S_never:\n",
    "                            S_never.remove(origin)\n",
    "                        if origin not in visited:\n",
    "                            to_visit.append(origin)\n",
    "                    else:\n",
    "                        S_sure.add(origin)\n",
    "                        if origin in S_never:\n",
    "                            S_never.remove(origin)\n",
    "    \n",
    "    return list(S_sure), list(S_may), list(S_never)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['S0', 'S1', 'S2', 'S3']\n",
      "Initialy declared actions: ['a', 'b', 'c']\n",
      "Transition from S0 with no action and targets ['S1', 'S2'] with weights [1, 1]\n",
      "Transition from S2 with no action and targets ['S3'] with weights [1]\n",
      "Transition from S1 with no action and targets ['S3'] with weights [1]\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - State S0 reward wasn't assigned, using zero as reward\n",
      "( 1 ) - State S1 reward wasn't assigned, using zero as reward\n",
      "( 2 ) - State S2 reward wasn't assigned, using zero as reward\n",
      "( 3 ) - State S3 reward wasn't assigned, using zero as reward\n"
     ]
    }
   ],
   "source": [
    "smaysurenever = run_mdp(path = \"mdp_examples//smaysurenever.mdp\", return_printer=True, print_transactions=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure:  ['S3', 'S2', 'S1']\n",
      "May:  []\n",
      "Never:  ['S0']\n"
     ]
    }
   ],
   "source": [
    "sure, may, never = find_states(smaysurenever, 'S3')\n",
    "\n",
    "print(\"Sure: \", sure)\n",
    "print(\"May: \", may)\n",
    "print(\"Never: \", never)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 13:\n",
    "\n",
    "A partir d'un MDP $ M = (S, \\text{Act}, P, \\iota_{\\text{init}}, AP, L) $ et une propriété $ P_{\\text{max}} (\\diamond s^*) $ avec $ s^* \\in S $ un état donné. Proposer une définition pour la matrice $ A $ et le vecteur $ b $ du programme linéaire $ A \\cdot x \\geq b $. Calculer $x$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 14\n",
    "\n",
    "Expliquer comment adapter les algorithmes de vérification de l'exercice 13 pour le calcul de la récompense attendue pour des modèles de récompense Markoviens (MC et MDP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 15\n",
    "Simulateur de lancers de pièces successifs pour simuler un dé à 6 faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 16\n",
    "Algorithme de SMC quantitatif - simulant un dé à 6 faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 17.1\n",
    "\n",
    "Implémenter l’algorithme de SMC qualitatif utilisant SPRT (Sequential Probability Ratio Test [Wald, 1945]) dans le programme de l’exercice 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 17.2\n",
    "On appelle $p_{ki}$ la probabilité d’obtenir le chiffre $i$ après $k$ lancers au maximum. Utiliser votre programme pour estimer $P(p_{ki} \\geq 0.1)$, $P(p_{ki} \\geq 0.14)$, $P(p_{ki} \\geq 0.16)$. Qu’observez-vous ? Utiliser $\\alpha = \\beta = 0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 17.3\n",
    "Tester sur le modèle du CRAPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 17.4\n",
    "Tester sur un MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modélisation Probabiliste et Apprentissage par Renforcement (Chapitre 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_10_infoia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
