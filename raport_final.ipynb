{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPAR - Rapport du rendu II\n",
    "**01/04/24**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Ce notebook présente l'explication, la démonstration et les résultats des outils développés dans le cadre du cours de MPAR. Les résultats de la première partie du rendu, discutés en cours, ne seront pas abordés ici mais seront uniquement utilisés comme référence.\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "1. Comprendre les concepts de **Model Checking** et de **SMC (Statistical Model Checking)** pour les **chaînes de Markov**.\n",
    "2. Étendre ces concepts aux **MDP (Markov Decision Process)**.\n",
    "3. Utiliser le **RL (Reinforcement Learning)** pour améliorer le model checking statistique.\n",
    "\n",
    "## Contenu\n",
    "1. **Changements dans le parser**\n",
    "2. **Chapitre 2, Vérification Probabiliste**\n",
    "3. **Chapitre 3, Modélisation Probabiliste et Apprentissage par Renforcement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Changements dans le parser\n",
    "\n",
    "Pour pouvoir utiliser des modèles .mdp avec des recompenses, il a fallut modifier notre parser. Pour ceci, les anciens modèles .mdp, sans récompenses, doivent continuer a fonctionnner sans altération.\n",
    "\n",
    "De cette façon, nous avons inclu la ligne optionnelle `Rewards` avec des entrées dans le format `s:r` avec s un état et r une récompense entière. L'exemple `states_with_rewards` montre ceci.\n",
    "\n",
    "Ceci a été fait en ajoutant l'entrée (defrewards) optionnele (?)  a la définition du programme, ayant `program: defstates defrewards? defactions transitions EOF;` et ayant la définition `defewrards : REWARDS ID ':' INT (',' ID ':' INT)* ';';`.\n",
    "\n",
    "Après, nous avons ajouté l'attribut `rewards` à la classe `gramPrintListener`, défini a travers les fonctions `enterDefrewards` et `update_rewards`.\n",
    "\n",
    "De cette façon, comme dans l'exemple vu en cours, il suffit d'importer la fonction `run` de l'archive `mdp.py` pour lire un fichier et creer un objet de la classe `gramPrintListener` qui contiendrat, en plus des attributs vus dans la prémière démostration, les attributs de récompenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import run as run_mdp # Fonction qui crée l'objet a être lu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple sans récompenses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['I', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6']\n",
      "Initialy declared actions: ['a']\n",
      "Transition from I with no action and targets ['T1', 'T2'] with weights [1, 1]\n",
      "Transition from T1 with no action and targets ['T3', 'T4'] with weights [1, 1]\n",
      "Transition from T2 with no action and targets ['T5', 'T6'] with weights [1, 1]\n",
      "Transition from T3 with no action and targets ['S1', 'T1'] with weights [1, 1]\n",
      "Transition from T4 with no action and targets ['S2', 'S3'] with weights [1, 1]\n",
      "Transition from T5 with no action and targets ['S4', 'S5'] with weights [1, 1]\n",
      "Transition from T6 with no action and targets ['S6', 'T2'] with weights [1, 1]\n",
      "Transition from S1 with no action and targets ['S1'] with weights [1]\n",
      "Transition from S2 with no action and targets ['S2'] with weights [1]\n",
      "Transition from S3 with no action and targets ['S3'] with weights [1]\n",
      "Transition from S4 with no action and targets ['S4'] with weights [1]\n",
      "Transition from S5 with no action and targets ['S5'] with weights [1]\n",
      "Transition from S6 with no action and targets ['S6'] with weights [1]\n",
      "\n",
      " ------- transactions df -------\n",
      "  Origin Action    I   T1   T2   T3   T4   T5   T6   S1   S2   S3   S4   S5  \\\n",
      "0      I     NA  NaN    1    1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "1     T1     NA  NaN  NaN  NaN    1    1  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "2     T2     NA  NaN  NaN  NaN  NaN  NaN    1    1  NaN  NaN  NaN  NaN  NaN   \n",
      "3     T3     NA  NaN    1  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN  NaN  NaN   \n",
      "4     T4     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1    1  NaN  NaN   \n",
      "5     T5     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1    1   \n",
      "6     T6     NA  NaN  NaN    1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "7     S1     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN  NaN  NaN   \n",
      "8     S2     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN  NaN   \n",
      "9     S3     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN   \n",
      "\n",
      "    S6  \n",
      "0  NaN  \n",
      "1  NaN  \n",
      "2  NaN  \n",
      "3  NaN  \n",
      "4  NaN  \n",
      "5  NaN  \n",
      "6    1  \n",
      "7  NaN  \n",
      "8  NaN  \n",
      "9  NaN  \n",
      "\n",
      " ------- transactions_prob df -------\n",
      "  Origin Action    I   T1   T2   T3   T4   T5   T6   S1   S2   S3   S4   S5  \\\n",
      "0      I     NA  0.0  0.5  0.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "1     T1     NA  0.0  0.0  0.0  0.5  0.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2     T2     NA  0.0  0.0  0.0  0.0  0.0  0.5  0.5  0.0  0.0  0.0  0.0  0.0   \n",
      "3     T3     NA  0.0  0.5  0.0  0.0  0.0  0.0  0.0  0.5  0.0  0.0  0.0  0.0   \n",
      "4     T4     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.5  0.5  0.0  0.0   \n",
      "5     T5     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.5  0.5   \n",
      "6     T6     NA  0.0  0.0  0.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "7     S1     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
      "8     S2     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
      "9     S3     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
      "\n",
      "    S6  \n",
      "0  0.0  \n",
      "1  0.0  \n",
      "2  0.0  \n",
      "3  0.0  \n",
      "4  0.0  \n",
      "5  0.0  \n",
      "6  0.5  \n",
      "7  0.0  \n",
      "8  0.0  \n",
      "9  0.0   \n",
      "\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - State I reward wasn't assigned, using zero as reward\n",
      "( 1 ) - State T1 reward wasn't assigned, using zero as reward\n",
      "( 2 ) - State T2 reward wasn't assigned, using zero as reward\n",
      "( 3 ) - State T3 reward wasn't assigned, using zero as reward\n",
      "( 4 ) - State T4 reward wasn't assigned, using zero as reward\n",
      "( 5 ) - State T5 reward wasn't assigned, using zero as reward\n",
      "( 6 ) - State T6 reward wasn't assigned, using zero as reward\n",
      "( 7 ) - State S1 reward wasn't assigned, using zero as reward\n",
      "( 8 ) - State S2 reward wasn't assigned, using zero as reward\n",
      "( 9 ) - State S3 reward wasn't assigned, using zero as reward\n",
      "( 10 ) - State S4 reward wasn't assigned, using zero as reward\n",
      "( 11 ) - State S5 reward wasn't assigned, using zero as reward\n",
      "( 12 ) - State S6 reward wasn't assigned, using zero as reward\n"
     ]
    }
   ],
   "source": [
    "simu_mc = run_mdp(path = \"prof_examples//simu-mc.mdp\", return_printer=True, print_transactions=True) # Exemple sans récompenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple avec récompenses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['S0', 'S1', 'S2']\n",
      "Initialy declared actions: ['a', 'b', 'c']\n",
      "Transition from S0 with no action and targets ['S1', 'S2'] with weights [5, 5]\n",
      "Transition from S1 with action b and targets ['S1', 'S0'] with weights [2, 8]\n",
      "Transition from S1 with action a and targets ['S2', 'S0', 'S1', 'S3'] with weights [1, 3, 6, 2]\n",
      "Transition from S2 with action c and targets ['S0', 'S1', 'S3'] with weights [5, 5, 10]\n",
      "Transition from S2 with action d and targets ['S0', 'S3'] with weights [5, 7]\n",
      "Transition from S3 with action e and targets ['S1', 'S2'] with weights [2, 2]\n",
      "\n",
      " ------- transactions df -------\n",
      "  Origin Action   S0   S1   S2    S3\n",
      "0     S0     NA  NaN    5    5   NaN\n",
      "1     S1      b    8    2  NaN   NaN\n",
      "2     S1      a    3    6    1   2.0\n",
      "3     S2      c    5    5  NaN  10.0\n",
      "4     S2      d    5  NaN  NaN   7.0\n",
      "5     S3      e  NaN    2    2   NaN\n",
      "\n",
      " ------- transactions_prob df -------\n",
      "  Origin Action        S0    S1        S2        S3\n",
      "0     S0     NA  0.000000  0.50  0.500000  0.000000\n",
      "1     S1      b  0.800000  0.20  0.000000  0.000000\n",
      "2     S1      a  0.250000  0.50  0.083333  0.166667\n",
      "3     S2      c  0.250000  0.25  0.000000  0.500000\n",
      "4     S2      d  0.416667  0.00  0.000000  0.583333\n",
      "5     S3      e  0.000000  0.50  0.500000  0.000000 \n",
      "\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - Undeclared state S3 targeted in transition: S1 with action a, declared automaticaly\n",
      "( 1 ) - Undeclared action in transition: S2 with action d, declared automaticaly\n",
      "( 2 ) - Undeclared action in transition: S3 with action e, declared automaticaly\n",
      "( 3 ) - State S3 reward wasn't assigned, using zero as reward\n"
     ]
    }
   ],
   "source": [
    "states_with_rewards = run_mdp(path = \"mdp_examples//states_with_rewards.mdp\", return_printer=True, print_transactions=True) # Exemple avec récompenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme montré dans les exemples précedents, à chaque état nous avons un reward attribué. \n",
    "\n",
    "Si la récompense n'a pas été définie, notre parser attribut une valeur de zero. \n",
    "\n",
    "Il est un dictionnaire et peut être appelé facilement par l'attribut `rewards` comme suit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states_with_reards.mdp {'S0': 1, 'S1': 10, 'S2': 15, 'S3': 0}\n",
      "simu_mc.mdp {'I': 0, 'T1': 0, 'T2': 0, 'T3': 0, 'T4': 0, 'T5': 0, 'T6': 0, 'S1': 0, 'S2': 0, 'S3': 0, 'S4': 0, 'S5': 0, 'S6': 0}\n"
     ]
    }
   ],
   "source": [
    "print('states_with_reards.mdp', states_with_rewards.rewards)\n",
    "print('simu_mc.mdp', simu_mc.rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vérification Probabiliste (Chapitre 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 12: \n",
    "Identification des ensembles $S_0$, $S_1$ et $S_?$ pour une propriété $P(\\diamond s)$ (fatalment $s$, où $s$ est un état)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "Nous avons décidé d'utiliser notre dataframe de probabilitées pour raisoner. L'idée geral de notre algorithme est:\n",
    "1. Au début, aucun état ne peut arriver ($S_{0} = all states $). Nous ne deplaçons les états seulement depuis $S_0$ vers les autres ensembles, jamais le chemin inverse où de $S_?$ vers $S_1$ où vice versa.\n",
    "2. L'état objectif satisfait toujours la propriété, alors on le déplace de $S_{0}$ pour $S_1$\n",
    "3. Maitenant, nous voulons trouver tous les états $S_1$. Pour ça, il faut faire une boucle.\n",
    "    - Nous calculons la probabilité totale de chaque couple (état, action) d'arriver aux états de $S_1$. \n",
    "    - Pour chaque état, entre toutes les actions possibles qu'il peut avoir, si la probabilité totale minimum est de 1, alors toutes sont égals a 1, donc il est déplacé pour $S_1$.\n",
    "    - Si $S_1$ a changé, nous recommençons le calcul. \n",
    "4. Maitenant, pour trouver les états $S_?$, nous refaisons une boucle.\n",
    "    - Idem pour la probabilité d'arrivé, mais cette fois-ci en $S_1$ où $S_?$.\n",
    "    - Ici, nous voulons l'ajouter a $S_?$ si au moins l'une de ces actions peut, peut-être, l'emmener a $S_?$ où $S_?$. Donc, il faut que la probabilité maximum d'un état soit plus grande que zéro.\n",
    "    - Si $S_?$ a changé, nous recommençons le calcul. \n",
    "\n",
    "\n",
    "#### Idée de l'algo\n",
    "\n",
    "L'idée de l'algorithme vient de la façon comme nous analisons un graphe pour trouver les ensembles. D'abord, nous començons toujours par $S_1$ et puis après par $S_?$. Aussi, a chaue fois nous regardons le graphe comme une seule chose au lieu de s'imaginer en marchant sur le graphe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_suremaynever_states(printer, target_state):\n",
    "    # Initialize sets for S_sure, S_may, and S_never\n",
    "    s_sure = set()\n",
    "    s_may = set()\n",
    "    s_never = set(printer.declared_states)\n",
    "    s_sure.add(target_state)\n",
    "    s_never.remove(target_state)\n",
    "    \n",
    "    stop = False\n",
    "    # First cycle to add s_sures\n",
    "    while not stop:\n",
    "        stop = True\n",
    "        probs = printer.transactions_prob.loc[:, ['Origin', 'Action']+list(s_sure)]\n",
    "        probs['P_sure'] = probs.loc[:, list(s_sure)].sum(axis=1)\n",
    "        for o in probs['Origin'].unique():\n",
    "            if probs.loc[probs['Origin']==o]['P_sure'].min() == 1: # It's sure if all actions lead to 100% prob of arriving to sure. \n",
    "                    if o in s_never:                               # Si un nouveau état est ajouté, on continue le cicle\n",
    "                        s_never.remove(o)\n",
    "                        s_sure.add(o)\n",
    "                        stop = False\n",
    "    stop = False\n",
    "    # First cycle to add s_sures\n",
    "    while not stop:\n",
    "        stop = True\n",
    "        probs = printer.transactions_prob.loc[:, ['Origin', 'Action']+list(s_may)+list(s_sure)]\n",
    "        probs['P_may'] = probs.loc[:, list(s_sure)+list(s_may)].sum(axis=1)\n",
    "        for o in probs['Origin'].unique():\n",
    "            if probs.loc[probs['Origin']==o]['P_may'].max() > 0: # It may arrive if at least one action have a probability of arriving to a may or sure. \n",
    "                    if o in s_never:                             # Si un nouveau état est ajouté, on continue le cicle\n",
    "                        s_never.remove(o)\n",
    "                        s_may.add(o)\n",
    "                        stop = False\n",
    "    \n",
    "    return list(s_sure), list(s_may), list(s_never)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['S0', 'S1', 'S2', 'S3']\n",
      "Initialy declared actions: ['a', 'b', 'c']\n",
      "Transition from S0 with action a and targets ['S1', 'S2'] with weights [1, 1]\n",
      "Transition from S0 with action b and targets ['S0'] with weights [1]\n",
      "Transition from S11 with no action and targets ['S2'] with weights [1, 1]\n",
      "Transition from S2 with no action and targets ['S3'] with weights [1]\n",
      "Transition from S1 with no action and targets ['S3'] with weights [1]\n",
      "Transition from S5 with no action and targets ['S6'] with weights [1]\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - Undeclared state in transition: S11, declared automaticaly\n",
      "( 1 ) - Undeclared state in transition: S5, declared automaticaly\n",
      "( 2 ) - Undeclared state S6 targeted in transition from S5 with NA, declared automaticaly\n",
      "( 3 ) - State S0 reward wasn't assigned, using zero as reward\n",
      "( 4 ) - State S1 reward wasn't assigned, using zero as reward\n",
      "( 5 ) - State S2 reward wasn't assigned, using zero as reward\n",
      "( 6 ) - State S3 reward wasn't assigned, using zero as reward\n",
      "( 7 ) - State S11 reward wasn't assigned, using zero as reward\n",
      "( 8 ) - State S5 reward wasn't assigned, using zero as reward\n",
      "( 9 ) - State S6 reward wasn't assigned, using zero as reward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "line 5:11 mismatched input ',' expecting {';', '+'}\n"
     ]
    }
   ],
   "source": [
    "smaysurenever = run_mdp(path = \"mdp_examples//smaysurenever.mdp\", return_printer=True, print_transactions=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['S2', 'S1', 'S11', 'S3'], ['S0'], ['S5', 'S6'])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_sure, S_may, S_never = segment_suremaynever_states(smaysurenever, 'S3')\n",
    "S_sure, sorted(S_may), sorted(S_never)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 13:\n",
    "\n",
    "A partir d'un MDP $ M = (S, \\text{Act}, P, \\iota_{\\text{init}}, AP, L) $ et une propriété $ P_{\\text{max}} (\\diamond s^*) $ avec $ s^* \\in S $ un état donné. Proposer une définition pour la matrice $ A $ et le vecteur $ b $ du programme linéaire $ A \\cdot x \\geq b $. Calculer $x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "Ici, nous voulons une solution gérale, qui marche pour des MDP mais aussi pour des MC. Pour ça, cest important de réetablire les vecteurs et matrices d'inéquations et équations a `None` quand nous n'avons pas d'équation où inéquation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "def solve_system(printer, S_may, S_sure):\n",
    "    '''\n",
    "    Fonctionne pour des MC et des MDP\n",
    "    '''\n",
    "    df = printer.transactions_prob\n",
    "    # Initialize lists for inequality and equality constraints\n",
    "    A_ub, b_ub, A_eq, b_eq = [], [], [], []\n",
    "    Ubfollower = []\n",
    "    # Iterate over source states in S_may\n",
    "    for source_state in S_may:\n",
    "        # Check if there is only one transition from the source state\n",
    "        if len(df.loc[df['Origin'] == source_state]) == 1:\n",
    "            # If only one transition, add it as an equality constraint\n",
    "            t = df.loc[df['Origin'] == source_state, S_may].copy()\n",
    "            t.loc[:, t.columns == source_state] -= 1  # Identity matrix substracted (A-I), but we invert later\n",
    "            A_eq.append(-t.values)                    # (A-I) becomes (I-A)\n",
    "            b_eq.append(np.sum(df.loc[df['Origin'] == source_state, S_sure].values, axis=1))\n",
    "        else:\n",
    "            # If multiple transitions, add them as inequality constraints\n",
    "            mask_state = df['Origin'] == source_state\n",
    "            Ubfollower.append(source_state)\n",
    "            ts = df.loc[mask_state, S_may].copy()\n",
    "            ts.loc[:, ts.columns == source_state] -=1\n",
    "            Ubfollower.append(list(df.loc[mask_state, 'Action']))\n",
    "            for i in range(len(df.loc[mask_state])):\n",
    "                A_ub.append(ts.values[i])\n",
    "                b_ub.append(-np.sum(df.loc[df['Origin'] == source_state, S_sure].values, axis=1)[i])\n",
    "    \n",
    "    c = np.ones(len(S_may)) # Objectif: Minimizer la somme des x de chaque état\n",
    "        \n",
    "    A_ub, b_ub, A_eq, b_eq = [None if not v else v for v in [A_ub, b_ub, A_eq, b_eq]]\n",
    "    A_eq, A_ub = [np.vstack(m) if m is not None else None for m in [A_eq, A_ub]]\n",
    "\n",
    "    # Solve the linear programming problem\n",
    "    print('--------- System constraints:')\n",
    "    print(f\"{Ubfollower=}\")\n",
    "    print(f\"{A_ub=}\")\n",
    "    print(f\"{b_ub=}\")\n",
    "    print(f\"{A_eq=}\")\n",
    "    print(f\"{b_eq=}\")\n",
    "    print(f\"{c=}\")\n",
    "\n",
    "    res = linprog(c=c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=(0,1))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abc_analysis(printer, state):\n",
    "    S_sure, S_may, S_never = segment_suremaynever_states(printer, state)\n",
    "    S_sure, sorted(S_may), sorted(S_never)\n",
    "\n",
    "    print('--------- S segments:')\n",
    "    print(f'{S_sure=}')\n",
    "    print(f'{S_may=}')\n",
    "    print(f'{S_never=}')\n",
    "\n",
    "    res = solve_system(printer, S_may, S_sure)\n",
    "\n",
    "    print('--------- Solution:')\n",
    "    print(f'{res.x=}')\n",
    "    print(f'{res.fun=}')\n",
    "    print(f'{res.success=}')\n",
    "    print(f'{res.slack=}')\n",
    "    print(f'{res.con=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testant pour une MC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['II', 'CI', 'AI', 'CC', 'CA', 'AC', 'AA', 'F', 'L']\n",
      "Initialy declared actions: ['NA']\n",
      "Transition from II with no action and targets ['CI', 'AI'] with weights [2, 1]\n",
      "Transition from CI with no action and targets ['CC'] with weights [1]\n",
      "Transition from CC with no action and targets ['F', 'L'] with weights [1, 1]\n",
      "Transition from CA with no action and targets ['F'] with weights [1]\n",
      "Transition from AC with no action and targets ['F'] with weights [1]\n",
      "Transition from AA with no action and targets ['L'] with weights [1]\n",
      "Transition from AI with no action and targets ['AA'] with weights [1]\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - State II reward wasn't assigned, using zero as reward\n",
      "( 1 ) - State CI reward wasn't assigned, using zero as reward\n",
      "( 2 ) - State AI reward wasn't assigned, using zero as reward\n",
      "( 3 ) - State CC reward wasn't assigned, using zero as reward\n",
      "( 4 ) - State CA reward wasn't assigned, using zero as reward\n",
      "( 5 ) - State AC reward wasn't assigned, using zero as reward\n",
      "( 6 ) - State AA reward wasn't assigned, using zero as reward\n",
      "( 7 ) - State F reward wasn't assigned, using zero as reward\n",
      "( 8 ) - State L reward wasn't assigned, using zero as reward\n"
     ]
    }
   ],
   "source": [
    "mc_c2p41 = run_mdp(path = \"mdp_examples//exemple_cours_c2p41_mc.mdp\", return_printer=True, print_transactions=False) # Exemple sans récompenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- S segments:\n",
      "S_sure=['F', 'AC', 'CA']\n",
      "S_may=['II', 'CI', 'CC']\n",
      "S_never=['L', 'AI', 'AA']\n",
      "--------- System constraints:\n",
      "Ubfollower=[]\n",
      "A_ub=None\n",
      "b_ub=None\n",
      "A_eq=array([[ 1.        , -0.66666667, -0.        ],\n",
      "       [-0.        ,  1.        , -1.        ],\n",
      "       [-0.        , -0.        ,  1.        ]])\n",
      "b_eq=[array([0.]), array([0.]), array([0.5])]\n",
      "c=array([1., 1., 1.])\n",
      "--------- Solution:\n",
      "res.x=array([0.33333333, 0.5       , 0.5       ])\n",
      "res.fun=1.3333333333333333\n",
      "res.success=True\n",
      "res.slack=array([], dtype=float64)\n",
      "res.con=array([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "abc_analysis(mc_c2p41, 'F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing for a simple MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['I', 'W', 'L']\n",
      "Initialy declared actions: ['a', 'b', 'c']\n",
      "Transition from I with action a and targets ['W', 'L'] with weights [1, 1]\n",
      "Transition from I with action b and targets ['W'] with weights [1]\n",
      "Transition from I with action c and targets ['L'] with weights [1]\n",
      "Transition from W with no action and targets ['W'] with weights [1]\n",
      "Transition from L with no action and targets ['L'] with weights [1]\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - State I reward wasn't assigned, using zero as reward\n",
      "( 1 ) - State W reward wasn't assigned, using zero as reward\n",
      "( 2 ) - State L reward wasn't assigned, using zero as reward\n"
     ]
    }
   ],
   "source": [
    "simple_mdp = run_mdp(path = \"mdp_examples//simple_mdp.mdp\", return_printer=True, print_transactions=False) # Exemple sans récompenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- S segments:\n",
      "S_sure=['W']\n",
      "S_may=['I']\n",
      "S_never=['L']\n",
      "--------- System constraints:\n",
      "Ubfollower=['I', ['a', 'b', 'c']]\n",
      "A_ub=array([[-1.],\n",
      "       [-1.],\n",
      "       [-1.]])\n",
      "b_ub=[-0.5, -1.0, -0.0]\n",
      "A_eq=None\n",
      "b_eq=None\n",
      "c=array([1.])\n",
      "--------- Solution:\n",
      "res.x=array([1.])\n",
      "res.fun=1.0\n",
      "res.success=True\n",
      "res.slack=array([0.5, 0. , 1. ])\n",
      "res.con=array([], dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "abc_analysis(simple_mdp, 'W')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing for another MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['II', 'CI', 'AI', 'CC', 'CA', 'AC', 'AA', 'F', 'L']\n",
      "Initialy declared actions: ['NA', 'a', 'p', 'c']\n",
      "Transition from II with action c and targets ['CI'] with weights [1]\n",
      "Transition from II with action p and targets ['CI', 'AI'] with weights [2, 1]\n",
      "Transition from II with action a and targets ['AI'] with weights [1]\n",
      "Transition from CI with action a and targets ['CA'] with weights [1]\n",
      "Transition from CI with action c and targets ['CC'] with weights [1]\n",
      "Transition from CI with action p and targets ['CC', 'CA'] with weights [2, 1]\n",
      "Transition from CC with no action and targets ['L', 'F'] with weights [1, 1]\n",
      "Transition from CA with no action and targets ['F'] with weights [1]\n",
      "Transition from F with no action and targets ['F'] with weights [1]\n",
      "Transition from L with no action and targets ['L'] with weights [1]\n",
      "Transition from AI with action c and targets ['AC'] with weights [1]\n",
      "Transition from AI with action a and targets ['AA'] with weights [1]\n",
      "Transition from AI with action p and targets ['AC', 'AA'] with weights [2, 1]\n",
      "Transition from AA with no action and targets ['L'] with weights [1]\n",
      "Transition from AC with no action and targets ['F'] with weights [1]\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - State II reward wasn't assigned, using zero as reward\n",
      "( 1 ) - State CI reward wasn't assigned, using zero as reward\n",
      "( 2 ) - State AI reward wasn't assigned, using zero as reward\n",
      "( 3 ) - State CC reward wasn't assigned, using zero as reward\n",
      "( 4 ) - State CA reward wasn't assigned, using zero as reward\n",
      "( 5 ) - State AC reward wasn't assigned, using zero as reward\n",
      "( 6 ) - State AA reward wasn't assigned, using zero as reward\n",
      "( 7 ) - State F reward wasn't assigned, using zero as reward\n",
      "( 8 ) - State L reward wasn't assigned, using zero as reward\n"
     ]
    }
   ],
   "source": [
    "mdp_c2p40 = run_mdp(path = \"mdp_examples//exemple_cours_c2p40_mdp.mdp\", return_printer=True, print_transactions=False) # Exemple sans récompenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- S segments:\n",
      "S_sure=['F', 'AC', 'CA']\n",
      "S_may=['II', 'AI', 'CI', 'CC']\n",
      "S_never=['L', 'AA']\n",
      "--------- System constraints:\n",
      "Ubfollower=['II', ['c', 'p', 'a'], 'AI', ['c', 'a', 'p'], 'CI', ['a', 'c', 'p']]\n",
      "A_ub=array([[-1.        ,  0.        ,  1.        ,  0.        ],\n",
      "       [-1.        ,  0.33333333,  0.66666667,  0.        ],\n",
      "       [-1.        ,  1.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -1.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -1.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -1.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        , -1.        ,  0.        ],\n",
      "       [ 0.        ,  0.        , -1.        ,  1.        ],\n",
      "       [ 0.        ,  0.        , -1.        ,  0.66666667]])\n",
      "b_ub=[-0.0, -0.0, -0.0, -1.0, -0.0, -0.6666666666666666, -1.0, -0.0, -0.3333333333333333]\n",
      "A_eq=array([[-0., -0., -0.,  1.]])\n",
      "b_eq=[array([0.5])]\n",
      "c=array([1., 1., 1., 1.])\n",
      "--------- Solution:\n",
      "res.x=array([1. , 1. , 1. , 0.5])\n",
      "res.fun=3.5\n",
      "res.success=True\n",
      "res.slack=array([-0.        , -0.        ,  0.        ,  0.        ,  1.        ,\n",
      "        0.33333333,  0.        ,  0.5       ,  0.33333333])\n",
      "res.con=array([0.])\n"
     ]
    }
   ],
   "source": [
    "abc_analysis(mdp_c2p40, 'F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 14\n",
    "\n",
    "Expliquer comment adapter les algorithmes de vérification de l'exercice 13 pour le calcul de la récompense attendue pour des modèles de récompense Markoviens (MC et MDP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 15\n",
    "Simulateur de lancers de pièces successifs pour simuler un dé à 6 faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 16\n",
    "Algorithme de SMC quantitatif - simulant un dé à 6 faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 17.1\n",
    "\n",
    "Implémenter l’algorithme de SMC qualitatif utilisant SPRT (Sequential Probability Ratio Test [Wald, 1945]) dans le programme de l’exercice 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 17.2\n",
    "On appelle $p_{ki}$ la probabilité d’obtenir le chiffre $i$ après $k$ lancers au maximum. Utiliser votre programme pour estimer $P(p_{ki} \\geq 0.1)$, $P(p_{ki} \\geq 0.14)$, $P(p_{ki} \\geq 0.16)$. Qu’observez-vous ? Utiliser $\\alpha = \\beta = 0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 17.3\n",
    "Tester sur le modèle du CRAPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 17.4\n",
    "Tester sur un MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modélisation Probabiliste et Apprentissage par Renforcement (Chapitre 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_10_infoia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
