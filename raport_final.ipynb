{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code complet, avec tous les archives, disponible sur git par le lien:\n",
    "`https://github.com/Powerloleu/TP_Renforcement.git`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPAR - Rapport du rendu II\n",
    "**01/04/24**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Ce notebook présente l'explication, la démonstration et les résultats des outils développés dans le cadre du cours de MPAR. Les résultats de la première partie du rendu, discutés en cours, ne seront pas abordés ici mais seront uniquement utilisés comme référence.\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "1. Comprendre les concepts de **Model Checking** et de **SMC (Statistical Model Checking)** pour les **chaînes de Markov**.\n",
    "2. Étendre ces concepts aux **MDP (Markov Decision Process)**.\n",
    "3. Utiliser le **RL (Reinforcement Learning)** pour améliorer le model checking statistique.\n",
    "\n",
    "## Contenu\n",
    "1. **Changements dans le parser**\n",
    "2. **Chapitre 2, Vérification Probabiliste**\n",
    "3. **Chapitre 3, Modélisation Probabiliste et Apprentissage par Renforcement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Changements dans le parser\n",
    "\n",
    "Pour pouvoir utiliser des modèles .mdp avec des recompenses, il a fallut modifier notre parser. Pour ceci, les anciens modèles .mdp, sans récompenses, doivent continuer a fonctionnner sans altération.\n",
    "\n",
    "De cette façon, nous avons inclu la ligne optionnelle `Rewards` avec des entrées dans le format `s:r` avec s un état et r une récompense entière. L'exemple `states_with_rewards` montre ceci.\n",
    "\n",
    "Ceci a été fait en ajoutant l'entrée (defrewards) optionnele (?)  a la définition du programme, ayant `program: defstates defrewards? defactions transitions EOF;` et ayant la définition `defewrards : REWARDS ID ':' INT (',' ID ':' INT)* ';';`.\n",
    "\n",
    "Après, nous avons ajouté l'attribut `rewards` à la classe `gramPrintListener`, défini a travers les fonctions `enterDefrewards` et `update_rewards`.\n",
    "\n",
    "De cette façon, comme dans l'exemple vu en cours, il suffit d'importer la fonction `run` de l'archive `mdp.py` pour lire un fichier et creer un objet de la classe `gramPrintListener` qui contiendrat, en plus des attributs vus dans la prémière démostration, les attributs de récompenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import run as run_mdp # Fonction qui crée l'objet a être lu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple sans récompenses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['I', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6']\n",
      "Initialy declared actions: ['a']\n",
      "Transition from I with no action and targets ['T1', 'T2'] with weights [1, 1]\n",
      "Transition from T1 with no action and targets ['T3', 'T4'] with weights [1, 1]\n",
      "Transition from T2 with no action and targets ['T5', 'T6'] with weights [1, 1]\n",
      "Transition from T3 with no action and targets ['S1', 'T1'] with weights [1, 1]\n",
      "Transition from T4 with no action and targets ['S2', 'S3'] with weights [1, 1]\n",
      "Transition from T5 with no action and targets ['S4', 'S5'] with weights [1, 1]\n",
      "Transition from T6 with no action and targets ['S6', 'T2'] with weights [1, 1]\n",
      "Transition from S1 with no action and targets ['S1'] with weights [1]\n",
      "Transition from S2 with no action and targets ['S2'] with weights [1]\n",
      "Transition from S3 with no action and targets ['S3'] with weights [1]\n",
      "Transition from S4 with no action and targets ['S4'] with weights [1]\n",
      "Transition from S5 with no action and targets ['S5'] with weights [1]\n",
      "Transition from S6 with no action and targets ['S6'] with weights [1]\n",
      "\n",
      " ------- transactions df -------\n",
      "  Origin Action    I   T1   T2   T3   T4   T5   T6   S1   S2   S3   S4   S5  \\\n",
      "0      I     NA  NaN    1    1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "1     T1     NA  NaN  NaN  NaN    1    1  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "2     T2     NA  NaN  NaN  NaN  NaN  NaN    1    1  NaN  NaN  NaN  NaN  NaN   \n",
      "3     T3     NA  NaN    1  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN  NaN  NaN   \n",
      "4     T4     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1    1  NaN  NaN   \n",
      "5     T5     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1    1   \n",
      "6     T6     NA  NaN  NaN    1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "7     S1     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN  NaN  NaN   \n",
      "8     S2     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN  NaN   \n",
      "9     S3     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN   \n",
      "\n",
      "    S6  \n",
      "0  NaN  \n",
      "1  NaN  \n",
      "2  NaN  \n",
      "3  NaN  \n",
      "4  NaN  \n",
      "5  NaN  \n",
      "6    1  \n",
      "7  NaN  \n",
      "8  NaN  \n",
      "9  NaN  \n",
      "\n",
      " ------- transactions_prob df -------\n",
      "  Origin Action    I   T1   T2   T3   T4   T5   T6   S1   S2   S3   S4   S5  \\\n",
      "0      I     NA  0.0  0.5  0.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "1     T1     NA  0.0  0.0  0.0  0.5  0.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2     T2     NA  0.0  0.0  0.0  0.0  0.0  0.5  0.5  0.0  0.0  0.0  0.0  0.0   \n",
      "3     T3     NA  0.0  0.5  0.0  0.0  0.0  0.0  0.0  0.5  0.0  0.0  0.0  0.0   \n",
      "4     T4     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.5  0.5  0.0  0.0   \n",
      "5     T5     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.5  0.5   \n",
      "6     T6     NA  0.0  0.0  0.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "7     S1     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
      "8     S2     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
      "9     S3     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
      "\n",
      "    S6  \n",
      "0  0.0  \n",
      "1  0.0  \n",
      "2  0.0  \n",
      "3  0.0  \n",
      "4  0.0  \n",
      "5  0.0  \n",
      "6  0.5  \n",
      "7  0.0  \n",
      "8  0.0  \n",
      "9  0.0   \n",
      "\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - State I reward wasn't assigned, using zero as reward\n",
      "( 1 ) - State T1 reward wasn't assigned, using zero as reward\n",
      "( 2 ) - State T2 reward wasn't assigned, using zero as reward\n",
      "( 3 ) - State T3 reward wasn't assigned, using zero as reward\n",
      "( 4 ) - State T4 reward wasn't assigned, using zero as reward\n",
      "( 5 ) - State T5 reward wasn't assigned, using zero as reward\n",
      "( 6 ) - State T6 reward wasn't assigned, using zero as reward\n",
      "( 7 ) - State S1 reward wasn't assigned, using zero as reward\n",
      "( 8 ) - State S2 reward wasn't assigned, using zero as reward\n",
      "( 9 ) - State S3 reward wasn't assigned, using zero as reward\n",
      "( 10 ) - State S4 reward wasn't assigned, using zero as reward\n",
      "( 11 ) - State S5 reward wasn't assigned, using zero as reward\n",
      "( 12 ) - State S6 reward wasn't assigned, using zero as reward\n"
     ]
    }
   ],
   "source": [
    "simu_mc = run_mdp(path = \"prof_examples//simu-mc.mdp\", return_printer=True, print_transactions=True) # Exemple sans récompenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple avec récompenses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['S0', 'S1', 'S2']\n",
      "Initialy declared actions: ['a', 'b', 'c']\n",
      "Transition from S0 with no action and targets ['S1', 'S2'] with weights [5, 5]\n",
      "Transition from S1 with action b and targets ['S1', 'S0'] with weights [2, 8]\n",
      "Transition from S1 with action a and targets ['S2', 'S0', 'S1', 'S3'] with weights [1, 3, 6, 2]\n",
      "Transition from S2 with action c and targets ['S0', 'S1', 'S3'] with weights [5, 5, 10]\n",
      "Transition from S2 with action d and targets ['S0', 'S3'] with weights [5, 7]\n",
      "Transition from S3 with action e and targets ['S1', 'S2'] with weights [2, 2]\n",
      "\n",
      " ------- transactions df -------\n",
      "  Origin Action   S0   S1   S2    S3\n",
      "0     S0     NA  NaN    5    5   NaN\n",
      "1     S1      b    8    2  NaN   NaN\n",
      "2     S1      a    3    6    1   2.0\n",
      "3     S2      c    5    5  NaN  10.0\n",
      "4     S2      d    5  NaN  NaN   7.0\n",
      "5     S3      e  NaN    2    2   NaN\n",
      "\n",
      " ------- transactions_prob df -------\n",
      "  Origin Action        S0    S1        S2        S3\n",
      "0     S0     NA  0.000000  0.50  0.500000  0.000000\n",
      "1     S1      b  0.800000  0.20  0.000000  0.000000\n",
      "2     S1      a  0.250000  0.50  0.083333  0.166667\n",
      "3     S2      c  0.250000  0.25  0.000000  0.500000\n",
      "4     S2      d  0.416667  0.00  0.000000  0.583333\n",
      "5     S3      e  0.000000  0.50  0.500000  0.000000 \n",
      "\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - Undeclared state S3 targeted in transition: S1 with action a, declared automaticaly\n",
      "( 1 ) - Undeclared action in transition: S2 with action d, declared automaticaly\n",
      "( 2 ) - Undeclared action in transition: S3 with action e, declared automaticaly\n",
      "( 3 ) - State S3 reward wasn't assigned, using zero as reward\n"
     ]
    }
   ],
   "source": [
    "states_with_rewards = run_mdp(path = \"mdp_examples//states_with_rewards.mdp\", return_printer=True, print_transactions=True) # Exemple avec récompenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme montré dans les exemples précedents, à chaque état nous avons un reward attribué. \n",
    "\n",
    "Si la récompense n'a pas été définie, notre parser attribut une valeur de zero. \n",
    "\n",
    "Il est un dictionnaire et peut être appelé facilement par l'attribut `rewards` comme suit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states_with_reards.mdp {'S0': 1, 'S1': 10, 'S2': 15, 'S3': 0}\n",
      "simu_mc.mdp {'I': 0, 'T1': 0, 'T2': 0, 'T3': 0, 'T4': 0, 'T5': 0, 'T6': 0, 'S1': 0, 'S2': 0, 'S3': 0, 'S4': 0, 'S5': 0, 'S6': 0}\n"
     ]
    }
   ],
   "source": [
    "print('states_with_reards.mdp', states_with_rewards.rewards)\n",
    "print('simu_mc.mdp', simu_mc.rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vérification Probabiliste (Chapitre 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 12: \n",
    "Identification des ensembles $S_0$, $S_1$ et $S_?$ pour une propriété $P(\\diamond s)$ (fatalment $s$, où $s$ est un état)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "Nous avons décidé d'utiliser notre dataframe de probabilitées pour raisoner. **Il marche avec des châines de Marcov et aussi des MDP**.\n",
    "\n",
    "##### L'idée geral de notre algorithme est:\n",
    "\n",
    "1. Au début, aucun état ne peut arriver: **$S_{0} = \\{s \\forall s \\in G$ \\}**. Nous ne deplaçons les états seulement depuis $S_0$ vers les autres ensembles, jamais le chemin inverse où de $S_?$ vers $S_1$ où vice versa.\n",
    "2. L'état objectif satisfait toujours la propriété, alors **on déplace $source$ de $S_{0}$ pour $S_1$**\n",
    "3. Maitenant, nous voulons **trouver tous les états $S_1$**. Pour ça, il faut faire **une boucle**.\n",
    "    - Nous calculons la probabilité totale de chaque couple (état, action) d'arriver aux états de $S_1$. \n",
    "    - Pour chaque état, entre toutes les actions possibles qu'il peut avoir, si la probabilité totale minimum est de 1, alors toutes sont égals a 1, donc il est déplacé pour $S_1$.\n",
    "    - Si $S_1$ a changé, nous recommençons le calcul. \n",
    "4. Maitenant, pour **trouver les états $S_?$**, nous refaisons **une boucle**.\n",
    "    - Idem pour la probabilité d'arrivé, mais cette fois-ci en $S_1$ où $S_?$.\n",
    "    - Ici, nous voulons l'ajouter a $S_?$ si au moins l'une de ces actions peut, peut-être, l'emmener a $S_?$ où $S_?$. Donc, il faut que la probabilité maximum d'un état soit plus grande que zéro.\n",
    "    - Si $S_?$ a changé, nous recommençons le calcul. \n",
    "\n",
    "\n",
    "#### Idée de l'algo\n",
    "\n",
    "L'idée de l'algorithme vient de la façon comme nous analisons un graphe pour trouver les ensembles. **D'abord, nous començons toujours par $S_1$ et puis après par $S_?$**. Aussi, a chaue fois nous regardons le graphe comme une seule chose au lieu de s'imaginer en marchant sur le graphe comme un walker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_suremaynever_states(printer, target_state):\n",
    "    # Initialize sets for S_sure, S_may, and S_never\n",
    "    s_sure = set()\n",
    "    s_may = set()\n",
    "    s_never = set(printer.declared_states)\n",
    "    s_sure.add(target_state)    # origin is sure\n",
    "    s_never.remove(target_state)\n",
    "    \n",
    "    stop = False\n",
    "    # First cycle to add s_sures\n",
    "    while not stop:\n",
    "        stop = True\n",
    "        probs = printer.transactions_prob.loc[:, ['Origin', 'Action']+list(s_sure)]\n",
    "        probs['P_sure'] = probs.loc[:, list(s_sure)].sum(axis=1)\n",
    "        for o in probs['Origin'].unique():\n",
    "            if probs.loc[probs['Origin']==o]['P_sure'].min() == 1: # It's sure if all actions lead to 100% prob of arriving to sure. (So the \"worst\" action has prob 1)\n",
    "                    if o in s_never:                               # If a new element is added, the df has changed, so we need to run again.\n",
    "                        s_never.remove(o)\n",
    "                        s_sure.add(o)\n",
    "                        stop = False\n",
    "    stop = False\n",
    "    # First cycle to add s_sures\n",
    "    while not stop:\n",
    "        stop = True\n",
    "        probs = printer.transactions_prob.loc[:, ['Origin', 'Action']+list(s_may)+list(s_sure)]\n",
    "        probs['P_may'] = probs.loc[:, list(s_sure)+list(s_may)].sum(axis=1)\n",
    "        for o in probs['Origin'].unique():\n",
    "            if probs.loc[probs['Origin']==o]['P_may'].max() > 0: # It may arrive if at least one action have a probability of arriving to a may or sure (so the \"best\" action have a probability different than zero). \n",
    "                    if o in s_never:                             # If a new element is added, the df has changed, so we need to run again.\n",
    "                        s_never.remove(o)\n",
    "                        s_may.add(o)\n",
    "                        stop = False\n",
    "    \n",
    "    return list(s_sure), list(s_may), list(s_never)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testant l'algorithme pour le graphe suivant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['S0', 'S1', 'S2', 'S3']\n",
      "Initialy declared actions: ['a', 'b', 'c']\n",
      "Transition from S0 with action a and targets ['S1', 'S2'] with weights [1, 1]\n",
      "Transition from S0 with action b and targets ['S0'] with weights [1]\n",
      "Transition from S11 with no action and targets ['S2'] with weights [1, 1]\n",
      "Transition from S2 with no action and targets ['S3'] with weights [1]\n",
      "Transition from S1 with no action and targets ['S3'] with weights [1]\n",
      "Transition from S5 with no action and targets ['S6'] with weights [1]\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - Undeclared state in transition: S11, declared automaticaly\n",
      "( 1 ) - Undeclared state in transition: S5, declared automaticaly\n",
      "( 2 ) - Undeclared state S6 targeted in transition from S5 with NA, declared automaticaly\n",
      "( 3 ) - State S0 reward wasn't assigned, using zero as reward\n",
      "( 4 ) - State S1 reward wasn't assigned, using zero as reward\n",
      "( 5 ) - State S2 reward wasn't assigned, using zero as reward\n",
      "( 6 ) - State S3 reward wasn't assigned, using zero as reward\n",
      "( 7 ) - State S11 reward wasn't assigned, using zero as reward\n",
      "( 8 ) - State S5 reward wasn't assigned, using zero as reward\n",
      "( 9 ) - State S6 reward wasn't assigned, using zero as reward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "line 5:11 mismatched input ',' expecting {';', '+'}\n"
     ]
    }
   ],
   "source": [
    "smaysurenever = run_mdp(path = \"mdp_examples//smaysurenever.mdp\", return_printer=True, print_transactions=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](smaysurenever.png)\n",
    "\n",
    "Image générée par l'archive `Plot_colors.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['S2', 'S1', 'S3', 'S11'], ['S0'], ['S5', 'S6'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_sure, S_may, S_never = segment_suremaynever_states(smaysurenever, 'S3')\n",
    "S_sure, sorted(S_may), sorted(S_never)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 13:\n",
    "\n",
    "A partir d'un MDP $ M = (S, \\text{Act}, P, \\iota_{\\text{init}}, AP, L) $ et une propriété $ P_{\\text{max}} (\\diamond s^*) $ avec $ s^* \\in S $ un état donné. Proposer une définition pour la matrice $ A $ et le vecteur $ b $ du programme linéaire $ A \\cdot x \\geq b $. Calculer $x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "Ici, nous voulons une solution gérale, qui marche pour des MDP mais aussi pour des MC. Pour ça, cest important de réetablire les vecteurs et matrices d'inéquations et équations a `None` quand nous n'avons pas d'équation où inéquation.\n",
    "\n",
    "Pour résoudre le problème de programmation linéaire décrit, nous utilisons la bibliothèque `scipy.optimize` avec la fonction `linprog`, visant à minimiser la fonction objectif, qui est la somme des valeurs des variables d'état. L'implémentation de l'algorithme suit plusieurs étapes clés pour définir les contraintes et formuler le problème de manière à pouvoir être résolu efficacement.\n",
    "\n",
    "- **Définition des Matrices et Vecteurs :**\n",
    "  - `A_eq` et `b_eq` : Ces matrices et vecteurs sont utilisés pour représenter les contraintes d'égalité dans le problème de programmation linéaire. Dans le contexte de MDPs ou MCs, ces contraintes correspondent aux transitions d'état où il y a une certitude sur le résultat de l'action, c'est-à-dire lorsque de l'état source, il existe une unique transition possible. Ces contraintes garantissent que la somme des probabilités des transitions pour ces états source spécifiques est égale à un vecteur déterminé par la somme des probabilités vers les états `S_sure`.\n",
    "  - `A_ub` et `b_ub` : Représentent les contraintes d'inégalité. Pour les états source avec plusieurs transitions possibles (dans `S_may`), ces contraintes sont définies pour refléter que la somme des probabilités de transition vers les états `S_may`, moins la probabilité de rester dans le même état, doit être inférieure ou égale aux probabilités de transition vers les états `S_sure`. Cela permet de gérer les incertitudes dans les transitions.\n",
    "\n",
    "- **Logique et Principe de l'Algorithme :**\n",
    "  L'algorithme parcourt tous les états source dans `S_may`. Pour chaque état, selon qu'il y ait une unique transition ou plusieurs, il applique une logique différente :\n",
    "  - Si une unique transition est possible, la contrainte est ajoutée comme une égalité.\n",
    "  - Si plusieurs transitions sont possibles, des contraintes d'inégalité sont ajoutées pour chaque transition possible.\n",
    "\n",
    "Cela est suivi par la définition de la fonction objectif `c`, où nous cherchons à minimiser la somme des variables d'état (représentant les probabilités). \n",
    "\n",
    "Ensuite, les matrices et vecteurs de contraintes sont préparés pour s'assurer qu'ils sont dans le bon format pour `linprog`, et le problème de programmation linéaire est résolu en appelant `linprog` avec les paramètres définis.\n",
    "\n",
    "Le résultat de `linprog` donne la distribution de probabilité optimale des états sous les contraintes données, minimisant ainsi la fonction objectif tout en satisfaisant les contraintes de transition entre les états.\n",
    "\n",
    "Cette approche fournit une méthode systématique et efficace pour résoudre des problèmes complexes de décision stochastique en utilisant la programmation linéaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "def solve_system(printer, S_may, S_sure):\n",
    "    '''\n",
    "    Résout un système de programmation linéaire défini par des contraintes sur les probabilités de transition entre les états d'un modèle de décision de Markov (MDP) ou d'une chaîne de Markov (MC).\n",
    "\n",
    "    Parameters:\n",
    "        printer (Printer): Un objet contenant les probabilités de transactions entre les états sous forme de DataFrame pandas.\n",
    "        S_may (list): Liste des états où il y a potentiellement plusieurs transitions possibles.\n",
    "        S_sure (list): Liste des états où une transition unique est sûre.\n",
    "\n",
    "    Returns:\n",
    "        res: Un objet OptimizeResult contenant le résultat de la résolution du problème de programmation linéaire.\n",
    "        Ce résultat inclut la distribution optimale des probabilités d'état, la valeur de la fonction objectif à l'optimisation, un booléen indiquant si l'optimisation a réussi,\n",
    "        et d'autres informations pertinentes sur le processus d'optimisation.\n",
    "\n",
    "    Cette fonction établit et résout un problème de programmation linéaire pour trouver la distribution optimale des probabilités d'état qui minimisent la somme des probabilités dans les états incertains (S_may),\n",
    "    tout en respectant les contraintes définies par les transitions sûres (S_sure) et les transitions potentielles (S_may) dans les modèles donnés.\n",
    "\n",
    "    Les contraintes d'égalité sont utilisées pour les états avec une transition unique et sûre, tandis que les contraintes d'inégalité sont appliquées aux états avec plusieurs transitions potentielles.\n",
    "    La fonction objectif vise à minimiser la somme des probabilités d'état dans S_may.\n",
    "\n",
    "    Exemple d'utilisation:\n",
    "        printer = Printer()  # Supposons que Printer est une classe définie ailleurs avec l'attribut transactions_prob.\n",
    "        S_may = ['état1', 'état2']\n",
    "        S_sure = ['état3', 'état4']\n",
    "        resultat = solve_system(printer, S_may, S_sure)\n",
    "        print(resultat)\n",
    "    '''\n",
    "     \n",
    "    df = printer.transactions_prob\n",
    "    # Initialize lists for inequality and equality constraints\n",
    "    A_ub, b_ub, A_eq, b_eq = [], [], [], []\n",
    "    Ubfollower = []\n",
    "    # Iterate over source states in S_may\n",
    "    for source_state in S_may:\n",
    "        # Check if there is only one transition from the source state\n",
    "        if len(df.loc[df['Origin'] == source_state]) == 1:\n",
    "            # If only one transition, add it as an equality constraint\n",
    "            t = df.loc[df['Origin'] == source_state, S_may].copy()\n",
    "            t.loc[:, t.columns == source_state] -= 1  # Identity matrix substracted (A-I), but we invert later\n",
    "            A_eq.append(-t.values)                    # (A-I) becomes (I-A)\n",
    "            b_eq.append(np.sum(df.loc[df['Origin'] == source_state, S_sure].values, axis=1))\n",
    "        else:\n",
    "            # If multiple transitions, add them as inequality constraints\n",
    "            mask_state = df['Origin'] == source_state\n",
    "            Ubfollower.append(source_state)\n",
    "            ts = df.loc[mask_state, S_may].copy()\n",
    "            ts.loc[:, ts.columns == source_state] -=1\n",
    "            Ubfollower.append(df.loc[mask_state, 'Action'].tolist())\n",
    "            for i in range(len(df.loc[mask_state])):\n",
    "                A_ub.append(ts.values[i])\n",
    "                b_ub.append(-np.sum(df.loc[df['Origin'] == source_state, S_sure].values, axis=1)[i])\n",
    "    \n",
    "    c = np.ones(len(S_may)) # Objectif: Minimizer la somme des x de chaque état\n",
    "        \n",
    "    A_ub, b_ub, A_eq, b_eq = [None if not v else v for v in [A_ub, b_ub, A_eq, b_eq]]\n",
    "    A_eq, A_ub = [np.vstack(m) if m is not None else None for m in [A_eq, A_ub]]\n",
    "\n",
    "    # Solve the linear programming problem\n",
    "    print('--------- System constraints:')\n",
    "    print(f\"{Ubfollower=}\")\n",
    "    print(f\"{A_ub=}\")\n",
    "    print(f\"{b_ub=}\")\n",
    "    print(f\"{A_eq=}\")\n",
    "    print(f\"{b_eq=}\")\n",
    "    print(f\"{c=}\")\n",
    "\n",
    "    res = linprog(c=c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=(0,1)) # Bounds établie des contraintes pour les valeurs de probabilité\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abc_analysis(printer, state):\n",
    "    S_sure, S_may, S_never = segment_suremaynever_states(printer, state)\n",
    "    S_sure, sorted(S_may), sorted(S_never)\n",
    "\n",
    "    print('--------- S segments:')\n",
    "    print(f'{S_sure=}')\n",
    "    print(f'{S_may=}')\n",
    "    print(f'{S_never=}')\n",
    "\n",
    "    res = solve_system(printer, S_may, S_sure)\n",
    "\n",
    "    print('--------- Solution:')\n",
    "    print(f'{res.x=}')\n",
    "    print(f'{res.fun=}')\n",
    "    print(f'{res.success=}')\n",
    "    print(f'{res.slack=}')\n",
    "    print(f'{res.con=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testant pour une MC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['II', 'CI', 'AI', 'CC', 'CA', 'AC', 'AA', 'F', 'L']\n",
      "Initialy declared actions: ['NA']\n",
      "Transition from II with no action and targets ['CI', 'AI'] with weights [2, 1]\n",
      "Transition from CI with no action and targets ['CC'] with weights [1]\n",
      "Transition from CC with no action and targets ['F', 'L'] with weights [1, 1]\n",
      "Transition from CA with no action and targets ['F'] with weights [1]\n",
      "Transition from AC with no action and targets ['F'] with weights [1]\n",
      "Transition from AA with no action and targets ['L'] with weights [1]\n",
      "Transition from AI with no action and targets ['AA'] with weights [1]\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - State II reward wasn't assigned, using zero as reward\n",
      "( 1 ) - State CI reward wasn't assigned, using zero as reward\n",
      "( 2 ) - State AI reward wasn't assigned, using zero as reward\n",
      "( 3 ) - State CC reward wasn't assigned, using zero as reward\n",
      "( 4 ) - State CA reward wasn't assigned, using zero as reward\n",
      "( 5 ) - State AC reward wasn't assigned, using zero as reward\n",
      "( 6 ) - State AA reward wasn't assigned, using zero as reward\n",
      "( 7 ) - State F reward wasn't assigned, using zero as reward\n",
      "( 8 ) - State L reward wasn't assigned, using zero as reward\n"
     ]
    }
   ],
   "source": [
    "mc_c2p41 = run_mdp(path = \"mdp_examples//exemple_cours_c2p41_mc.mdp\", return_printer=True, print_transactions=False) # Exemple sans récompenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](exemple_cours_c2p41_mc.png)\n",
    "\n",
    "Image générée par l'archive `Plot_colors.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- S segments:\n",
      "S_sure=['AC', 'F', 'CA']\n",
      "S_may=['CC', 'CI', 'II']\n",
      "S_never=['AI', 'L', 'AA']\n",
      "--------- System constraints:\n",
      "Ubfollower=[]\n",
      "A_ub=None\n",
      "b_ub=None\n",
      "A_eq=array([[ 1.        , -0.        , -0.        ],\n",
      "       [-1.        ,  1.        , -0.        ],\n",
      "       [-0.        , -0.66666667,  1.        ]])\n",
      "b_eq=[array([0.5]), array([0.]), array([0.])]\n",
      "c=array([1., 1., 1.])\n",
      "--------- Solution:\n",
      "res.x=array([0.5       , 0.5       , 0.33333333])\n",
      "res.fun=1.3333333333333333\n",
      "res.success=True\n",
      "res.slack=array([], dtype=float64)\n",
      "res.con=array([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "abc_analysis(mc_c2p41, 'F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interprétation du résultat - MC:\n",
    "\n",
    "Ici, dans une châine de Marcov, ce qui nous intérèsse est surtout le vecteur `res.x` qui rend les probabilitées des $S_?$. Nous avons:\n",
    "\n",
    "$$P_{L} = 0.33, P_{AI} = 0.5, P_{AA} = 0.5$$\n",
    "\n",
    "Le vecteur `res.con` étant zéro pour chaque inéquation montre que l'égalité a toujours été atteinte. Si ce n'était pas le cas, il n'y aurait pas de solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testant pour un MDP simple\n",
    "\n",
    "Pour le premier teste, nous avons choisi un MDP très simple pour que l'explication soit plus compréhensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['I', 'W', 'L']\n",
      "Initialy declared actions: ['a', 'b', 'c']\n",
      "Transition from I with action a and targets ['W', 'L'] with weights [1, 1]\n",
      "Transition from I with action b and targets ['W'] with weights [1]\n",
      "Transition from I with action c and targets ['L'] with weights [1]\n",
      "Transition from W with no action and targets ['W'] with weights [1]\n",
      "Transition from L with no action and targets ['L'] with weights [1]\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - State I reward wasn't assigned, using zero as reward\n",
      "( 1 ) - State W reward wasn't assigned, using zero as reward\n",
      "( 2 ) - State L reward wasn't assigned, using zero as reward\n"
     ]
    }
   ],
   "source": [
    "simple_mdp = run_mdp(path = \"mdp_examples//simple_mdp.mdp\", return_printer=True, print_transactions=False) # Exemple sans récompenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](simple_mdp.png)\n",
    "\n",
    "Image générée par l'archive `Plot_colors.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- S segments:\n",
      "S_sure=['W']\n",
      "S_may=['I']\n",
      "S_never=['L']\n",
      "--------- System constraints:\n",
      "Ubfollower=['I', ['a', 'b', 'c']]\n",
      "A_ub=array([[-1.],\n",
      "       [-1.],\n",
      "       [-1.]])\n",
      "b_ub=[-0.5, -1.0, -0.0]\n",
      "A_eq=None\n",
      "b_eq=None\n",
      "c=array([1.])\n",
      "--------- Solution:\n",
      "res.x=array([1.])\n",
      "res.fun=1.0\n",
      "res.success=True\n",
      "res.slack=array([0.5, 0. , 1. ])\n",
      "res.con=array([], dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "abc_analysis(simple_mdp, 'W')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interprétation du résultat - MDP simple:\n",
    "\n",
    "Dans ce MDP, nous n'avos pas de contraintes d'égalitées. Ceci est du a cause du fait que l'unique état dans $S_{may}$ a des actions a choisir.\n",
    "\n",
    "A nouveau, le vecteur x réprésente la probabilitée des états dans $S_{?}$. Dans les cas où il faut choisir un adversaire, cette probabilitée est la maximale (en choisissant le meilleur adversaire). Dans notre cas, elle est de $1$.\n",
    "\n",
    "Le vecteur `res.slack` nous donne l'information sur quel adversaire a été choisi. Il répresente la différence dans chaque équation d'inégalité. **Pour les adversaires qui maximizent la probabilité, leur contrainte est active et donc la différence vaut zéro.** \n",
    "\n",
    "Pour interpréter qu'elle action est associé a la valeur du vecteur, nous avons le vecteur de strings `Ubfollower`. Avec les deux, nous povons voir que:\n",
    "\n",
    "$$Slack_{I(a)} = 0.5, Slack_{I(b)} = 0, Slack_{I(c)} = 1,$$\n",
    "\n",
    "Ce qui montre que l'adversaire optimale choisi $b$ dans l'état $I$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teste pour un MDP plus complexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['II', 'CI', 'AI', 'CC', 'CA', 'AC', 'AA', 'F', 'L']\n",
      "Initialy declared actions: ['NA', 'a', 'p', 'c']\n",
      "Transition from II with action c and targets ['CI'] with weights [1]\n",
      "Transition from II with action p and targets ['CI', 'AI'] with weights [2, 1]\n",
      "Transition from II with action a and targets ['AI'] with weights [1]\n",
      "Transition from CI with action a and targets ['CA'] with weights [1]\n",
      "Transition from CI with action c and targets ['CC'] with weights [1]\n",
      "Transition from CI with action p and targets ['CC', 'CA'] with weights [2, 1]\n",
      "Transition from CC with no action and targets ['L', 'F'] with weights [1, 1]\n",
      "Transition from CA with no action and targets ['F'] with weights [1]\n",
      "Transition from F with no action and targets ['F'] with weights [1]\n",
      "Transition from L with no action and targets ['L'] with weights [1]\n",
      "Transition from AI with action c and targets ['AC'] with weights [1]\n",
      "Transition from AI with action a and targets ['AA'] with weights [1]\n",
      "Transition from AI with action p and targets ['AC', 'AA'] with weights [2, 1]\n",
      "Transition from AA with no action and targets ['L'] with weights [1]\n",
      "Transition from AC with no action and targets ['F'] with weights [1]\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - State II reward wasn't assigned, using zero as reward\n",
      "( 1 ) - State CI reward wasn't assigned, using zero as reward\n",
      "( 2 ) - State AI reward wasn't assigned, using zero as reward\n",
      "( 3 ) - State CC reward wasn't assigned, using zero as reward\n",
      "( 4 ) - State CA reward wasn't assigned, using zero as reward\n",
      "( 5 ) - State AC reward wasn't assigned, using zero as reward\n",
      "( 6 ) - State AA reward wasn't assigned, using zero as reward\n",
      "( 7 ) - State F reward wasn't assigned, using zero as reward\n",
      "( 8 ) - State L reward wasn't assigned, using zero as reward\n"
     ]
    }
   ],
   "source": [
    "mdp_c2p40 = run_mdp(path = \"mdp_examples//exemple_cours_c2p40_mdp.mdp\", return_printer=True, print_transactions=False) # Exemple sans récompenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](exemple_cours_c2p40_mdp.png)\n",
    "\n",
    "Image générée par l'archive `Plot_colors.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- S segments:\n",
      "S_sure=['AC', 'F', 'CA']\n",
      "S_may=['CC', 'CI', 'II', 'AI']\n",
      "S_never=['L', 'AA']\n",
      "--------- System constraints:\n",
      "Ubfollower=['CI', ['a', 'c', 'p'], 'II', ['c', 'p', 'a'], 'AI', ['c', 'a', 'p']]\n",
      "A_ub=array([[ 0.        , -1.        ,  0.        ,  0.        ],\n",
      "       [ 1.        , -1.        ,  0.        ,  0.        ],\n",
      "       [ 0.66666667, -1.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  1.        , -1.        ,  0.        ],\n",
      "       [ 0.        ,  0.66666667, -1.        ,  0.33333333],\n",
      "       [ 0.        ,  0.        , -1.        ,  1.        ],\n",
      "       [ 0.        ,  0.        ,  0.        , -1.        ],\n",
      "       [ 0.        ,  0.        ,  0.        , -1.        ],\n",
      "       [ 0.        ,  0.        ,  0.        , -1.        ]])\n",
      "b_ub=[-1.0, -0.0, -0.3333333333333333, -0.0, -0.0, -0.0, -1.0, -0.0, -0.6666666666666666]\n",
      "A_eq=array([[ 1., -0., -0., -0.]])\n",
      "b_eq=[array([0.5])]\n",
      "c=array([1., 1., 1., 1.])\n",
      "--------- Solution:\n",
      "res.x=array([0.5, 1. , 1. , 1. ])\n",
      "res.fun=3.5\n",
      "res.success=True\n",
      "res.slack=array([ 0.        ,  0.5       ,  0.33333333,  0.        , -0.        ,\n",
      "       -0.        ,  0.        ,  1.        ,  0.33333333])\n",
      "res.con=array([0.])\n"
     ]
    }
   ],
   "source": [
    "abc_analysis(mdp_c2p40, 'F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interprétation du résultat - MDP complexe:\n",
    "\n",
    "Dans ce MDP, nous n'avos:\n",
    "- 9 contraintes d'inégalitées, 3 pour chaque état $\\{II, CI, AI\\}$.\n",
    "- 1 contrainte d'égalité pour l'état $CC$.\n",
    "A nouveau, le vecteur x réprésente la probabilitée des états dans $S_{?}$. Dans notre cas:\n",
    "\n",
    "$$P_{II} = 1,  P_{CI}, P_{AI} = 1, P_{CC} = 1$$\n",
    "\n",
    "Ce qui signifie que, avec un adversaire optimal, nous arrivons toujours en $F$.\n",
    "\n",
    "Interprétant le vecteur `res.slack` et `Ubfollower`, nous avons: \n",
    "- Adversaire optimal de $II : \\{c, p, a\\} \\to$ N'importe lequel de ces choix peux former un adversaire optimal, puisque les 3 contraintes sont actives (égales a zéro).\n",
    "- Adversaire optimal de $CI: a \\to$ L'état $a$ est l'unique qui optimize, donc notre adversaire doit toujours choisir $c$ dans l'état $CI$.\n",
    "- Adversaire optimal $AI: c \\to$ Idem que pour l'état $CI$ avec $c$.\n",
    "\n",
    "Cette solution est intéressante parce que elle retourne toutes les résultats d'adversaires optimales possibles quand bien interprété. \n",
    "\n",
    "### Observation sur le format du résultat:\n",
    "Nous avons préféré laisser un résultat plus \"brute\" pour nous obliger a bien comprendre la méthode de minimization derrière l'algorithme. Bien sûr, avec les vecteurs retournées, on aurait pû créer une solution directe facilement interprétable. Mais ça n'ajouté pas grand chose a la compréhension du problème."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 14\n",
    "\n",
    "Expliquer comment adapter les algorithmes de vérification de l'exercice 13 pour le calcul de la récompense attendue pour des modèles de récompense Markoviens (MC et MDP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul de l'Espérance de Récompense pour les MC et MDP\n",
    "\n",
    "Pour calculer l'espérance de récompense dans des modèles Markoviens, tels que les chaînes de Markov (MC) et les processus de décision de Markov (MDP), nous adoptons une approche basée sur les équations et inéquations qui intègrent les probabilités de transition entre états et les récompenses associées à ces transitions.\n",
    "\n",
    "#### Chaînes de Markov (MC)\n",
    "\n",
    "Pour une chaîne de Markov, l'espérance de récompense d'un état donné, notée $X_{\\text{état}}$, est calculée via l'équation suivante :\n",
    "\n",
    "$$X_{\\text{état}} = \\text{reward}_{\\text{état}} + \\sum_{\\text{tous états}} (\\text{probabilité de transition vers cet état} \\times X_{\\text{autres états}})$$\n",
    "\n",
    "Ici, $X_{\\text{état}}$ représente l'espérance de récompense pour l'état concerné, $\\text{reward}_{\\text{état}}$ est la récompense immédiate reçue en étant dans cet état, et la somme calcule l'espérance de récompense pondérée par les probabilités de transition vers tous les autres états.\n",
    "\n",
    "#### Processus de Décision de Markov (MDP)\n",
    "\n",
    "Dans le contexte des MDP, où les décisions (actions) influencent les transitions, l'espérance de récompense pour chaque action spécifique dans chaque état est définie par l'inéquation :\n",
    "\n",
    "$$X_{\\text{état}}^{\\text{action}} \\leq \\text{reward}_{\\text{état}} + \\sum_{\\text{tous états}} (\\text{probabilité de transition vers cet état en utilisant l'action} \\times X_{\\text{autres états}})$$\n",
    "\n",
    "$X_{\\text{état}}^{\\text{action}}$ désigne ici l'espérance de récompense pour un état donné lors de l'exécution d'une action spécifique. Cette formulation sous forme d'inéquation reflète le principe selon lequel, pour un MDP, l'espérance de récompense de choisir une action spécifique est contrainte par la somme de la récompense immédiate et de l'espérance de récompense des états suivants, cette dernière étant pondérée par les probabilités de transition utilisant l'action choisie.\n",
    "\n",
    "L'emploi d'inéquations pour les MDP illustre le processus de choix et d'optimisation des actions : l'objectif est de déterminer la politique (c.-à-d., la sélection des actions dans chaque état) qui maximise l'espérance globale de récompense. Cela implique de maximiser la valeur de $X_{\\text{état}}$ pour chaque état par le biais d'une sélection optimale d'actions.\n",
    "\n",
    "Cette méthode offre un cadre pour modéliser et calculer l'espérance de récompense dans les MC et MDP, prenant en compte la structure décisionnelle et les transitions probabilistes entre états, et vise à optimiser les décisions en maximisant la récompense attendue.\n",
    "\n",
    "L'implémentant sur python, nous avons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "def solve_system_with_rewards(printer, S_may, S_sure):     \n",
    "    df = printer.transactions_prob\n",
    "    # Initialize lists for inequality and equality constraints\n",
    "    A_ub, b_ub, A_eq, b_eq = [], [], [], []\n",
    "    Ubfollower = []\n",
    "    # Iterate over source states in S_may\n",
    "    for source_state in S_may:\n",
    "        # Check if there is only one transition from the source state\n",
    "        if len(df.loc[df['Origin'] == source_state]) == 1:\n",
    "            # If only one transition, add it as an equality constraint\n",
    "            t = df.loc[df['Origin'] == source_state, S_may].copy()\n",
    "            t.loc[:, t.columns == source_state] -= 1  # Reward of the state\n",
    "            A_eq.append(-t.values * np.array([printer.rewards[s] for s in S_may]))    # + expected reward * probability of other states \n",
    "            b_eq.append(printer.rewards[source_state])  # State contraint reward\n",
    "        else:\n",
    "            # If multiple transitions, add them as inequality constraints\n",
    "            mask_state = df['Origin'] == source_state\n",
    "            Ubfollower.append(source_state)\n",
    "            ts = df.loc[mask_state, S_may].copy()\n",
    "            ts.loc[:, ts.columns == source_state] -=1 # Reward of the state\n",
    "            Ubfollower.append(df.loc[mask_state, 'Action'].tolist())\n",
    "            for i in range(len(df.loc[mask_state])):\n",
    "                A_ub.append(-ts.values[i]*np.array([printer.rewards[s] for s in S_may])) # sommatory of probs * rewards with -x added before\n",
    "                b_ub.append(printer.rewards[source_state])\n",
    "    \n",
    "    c = -np.ones(len(S_may)) # Objectif: Minimizer l'éspérance de récompense dans chaque état \n",
    "        \n",
    "    A_ub, b_ub, A_eq, b_eq = [None if not v else v for v in [A_ub, b_ub, A_eq, b_eq]]\n",
    "    A_eq, A_ub = [np.vstack(m) if m is not None else None for m in [A_eq, A_ub]]\n",
    "\n",
    "    # Solve the linear programming problem\n",
    "    print('--------- System constraints:')\n",
    "    print(f\"{Ubfollower=}\")\n",
    "    print(f\"{A_ub=}\")\n",
    "    print(f\"{b_ub=}\")\n",
    "    print(f\"{A_eq=}\")\n",
    "    print(f\"{b_eq=}\")\n",
    "    print(f\"{c=}\")\n",
    "\n",
    "    res = linprog(c=c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=(0, None)) # Ici, il faut juste que les récompenses soyent positives.\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation:\n",
    "\n",
    "Cette version est simplifié, elle calcule la récompense espérée d'arriver dans un état $S_{sure}$. Ça veut dire qu'elle ne comptabilize pas les récompenses entre le premier état $S_{sure}$ et l'état final souhaité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_analysis(printer, state):\n",
    "    S_sure, S_may, S_never = segment_suremaynever_states(printer, state)\n",
    "    S_sure, sorted(S_may), sorted(S_never)\n",
    "\n",
    "    print('--------- S segments:')\n",
    "    print(f'{S_sure=}')\n",
    "    print(f'{S_may=}')\n",
    "    print(f'{S_never=}')\n",
    "\n",
    "    res = solve_system_with_rewards(printer, S_may, S_sure)\n",
    "\n",
    "    print('--------- Solution:')\n",
    "    print(f'{res.x=}')\n",
    "    print(f'{res.fun=}')\n",
    "    print(f'{res.success=}')\n",
    "    print(f'{res.slack=}')\n",
    "    print(f'{res.con=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple d'implémentation de l'algo avec des récompenses dans un MC\n",
    "\n",
    "Ici, supposons que nous avons un board avec des cases de S0 à S3 et puis F. À chaque tour, on se déplace vers n'importe quelle case avec une probabilité unniforme (sauf la case où la pièce est, en autres mots elle se déplace toujours). Ici, la question qu'on veut répondre est combien de tours il faut en moyenne pour arriver en F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['S0', 'S1', 'S2', 'S3', 'F']\n",
      "Initialy declared actions: ['b']\n",
      "Transition from S0 with action b and targets ['S1', 'S2', 'S3', 'F'] with weights [1, 1, 1, 1]\n",
      "Transition from S1 with action b and targets ['S0', 'S2', 'S3', 'F'] with weights [1, 1, 1, 1]\n",
      "Transition from S2 with action b and targets ['S0', 'S1', 'S3', 'F'] with weights [1, 1, 1, 1]\n",
      "Transition from S3 with action b and targets ['S0', 'S1', 'S2', 'F'] with weights [1, 1, 1, 1]\n",
      "Transition from F with no action and targets ['F'] with weights [1]\n",
      "\n",
      " ------- transactions df -------\n",
      "  Origin Action   S0   S1   S2   S3  F\n",
      "0     S0      b  NaN    1    1    1  1\n",
      "1     S1      b    1  NaN    1    1  1\n",
      "2     S2      b    1    1  NaN    1  1\n",
      "3     S3      b    1    1    1  NaN  1\n",
      "4      F     NA  NaN  NaN  NaN  NaN  1\n",
      "\n",
      " ------- transactions_prob df -------\n",
      "  Origin Action    S0    S1    S2    S3     F\n",
      "0     S0      b  0.00  0.25  0.25  0.25  0.25\n",
      "1     S1      b  0.25  0.00  0.25  0.25  0.25\n",
      "2     S2      b  0.25  0.25  0.00  0.25  0.25\n",
      "3     S3      b  0.25  0.25  0.25  0.00  0.25\n",
      "4      F     NA  0.00  0.00  0.00  0.00  1.00 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "line 6:28 extraneous input '1' expecting {';', '+'}\n"
     ]
    }
   ],
   "source": [
    "mc_chemin_plus_court_arriver_en_f = run_mdp(path = \"mdp_examples//mc_chemin_plus_court_arriver_en_f.mdp\", return_printer=True, print_transactions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](mc_chemin_plus_court_arriver_en_f.png)\n",
    "\n",
    "Image générée par l'archive `Plot_colors.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'S0': 1, 'S1': 1, 'S2': 1, 'S3': 1, 'F': 0}\n"
     ]
    }
   ],
   "source": [
    "print(mc_chemin_plus_court_arriver_en_f.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- S segments:\n",
      "S_sure=['F']\n",
      "S_may=['S2', 'S0', 'S1', 'S3']\n",
      "S_never=[]\n",
      "--------- System constraints:\n",
      "Ubfollower=[]\n",
      "A_ub=None\n",
      "b_ub=None\n",
      "A_eq=array([[ 1.  , -0.25, -0.25, -0.25],\n",
      "       [-0.25,  1.  , -0.25, -0.25],\n",
      "       [-0.25, -0.25,  1.  , -0.25],\n",
      "       [-0.25, -0.25, -0.25,  1.  ]])\n",
      "b_eq=[1, 1, 1, 1]\n",
      "c=array([-1., -1., -1., -1.])\n",
      "--------- Solution:\n",
      "res.x=array([4., 4., 4., 4.])\n",
      "res.fun=-16.0\n",
      "res.success=True\n",
      "res.slack=array([], dtype=float64)\n",
      "res.con=array([0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "reward_analysis(mc_chemin_plus_court_arriver_en_f, 'F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interprétation du résultat\n",
    "\n",
    "Il est clairment correct. En première place parce que toutes les cases ont la même éspérance, ce qui est attendu en raison de la symétrie du problème. Pour vérifier si c'est bien 4, nous pouvons englober tous les états équivalents dans un état $S$ qui a probabilité $3/4$ de rester sur $S$ et $1/4$ de passer en $F$. De cette façon, l'ésperance de tours avant d'arriver en $F$ est de:\n",
    "\n",
    "$$E(F) = 1\\cdot \\frac{1}{4} + 2 \\cdot \\frac{3}{4}\\cdot \\frac{1}{4} + ... + n \\cdot \\frac{3^{n-1}}{4^{n-1}} \\cdot \\frac{1}{4} $$\n",
    "$$E(F) = \\sum_{i=0}^{i=n} i \\cdot \\frac{3^{i-1}}{4^{i-1}} \\cdot \\frac{1}{4},n \\to +\\inf $$\n",
    "$$E(F) - \\frac{3}{4}E(F) = \\sum_{i=0}^{i=1} 1 \\cdot \\frac{3^{i-1}}{4^{i-1}} \\cdot \\frac{1}{4},n \\to +\\inf $$\n",
    "$$\\frac{1}{4}E(F) = \\frac{\\frac{1}{4}}{1- \\frac{3}{4}} $$\n",
    "$$E(F) = 4$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple d'implémentation de l'algo avec des récompenses dans un MDP\n",
    "\n",
    "Ici, supposons que nous avons un board avec des cases de S0 à S3 et puis F. À chaque tour, nous choisissons de faire un pas en avant (F est après S3, c'est la dernière case) où de se déplacer vers n'importe quelle case avec une probabilité unniforme (sauf la case où la pièce est, en autres mots elle se déplace toujours). Ici, la question qu'on veut répondre est quel adversaire choisir pour optimizer le nombre de tours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['S0', 'S1', 'S2', 'S3', 'F']\n",
      "Initialy declared actions: ['a', 'b']\n",
      "Transition from S0 with action a and targets ['S1'] with weights [1]\n",
      "Transition from S0 with action b and targets ['S1', 'S2', 'S3', 'F'] with weights [1, 1, 1, 1]\n",
      "Transition from S1 with action a and targets ['S2'] with weights [1]\n",
      "Transition from S1 with action b and targets ['S0', 'S2', 'S3', 'F'] with weights [1, 1, 1, 1]\n",
      "Transition from S2 with action a and targets ['S3'] with weights [1]\n",
      "Transition from S2 with action b and targets ['S0', 'S1', 'S3', 'F'] with weights [1, 1, 1, 1]\n",
      "Transition from S3 with action a and targets ['F'] with weights [1]\n",
      "Transition from S3 with action b and targets ['S0', 'S1', 'S2', 'F'] with weights [1, 1, 1, 1]\n",
      "Transition from F with no action and targets ['F'] with weights [1]\n",
      "\n",
      " ------- transactions df -------\n",
      "  Origin Action   S0   S1   S2   S3    F\n",
      "0     S0      a  NaN    1  NaN  NaN  NaN\n",
      "1     S0      b  NaN    1    1    1    1\n",
      "2     S1      a  NaN  NaN    1  NaN  NaN\n",
      "3     S1      b    1  NaN    1    1    1\n",
      "4     S2      a  NaN  NaN  NaN    1  NaN\n",
      "5     S2      b    1    1  NaN    1    1\n",
      "6     S3      a  NaN  NaN  NaN  NaN    1\n",
      "7     S3      b    1    1    1  NaN    1\n",
      "8      F     NA  NaN  NaN  NaN  NaN    1\n",
      "\n",
      " ------- transactions_prob df -------\n",
      "  Origin Action    S0    S1    S2    S3     F\n",
      "0     S0      a  0.00  1.00  0.00  0.00  0.00\n",
      "1     S0      b  0.00  0.25  0.25  0.25  0.25\n",
      "2     S1      a  0.00  0.00  1.00  0.00  0.00\n",
      "3     S1      b  0.25  0.00  0.25  0.25  0.25\n",
      "4     S2      a  0.00  0.00  0.00  1.00  0.00\n",
      "5     S2      b  0.25  0.25  0.00  0.25  0.25\n",
      "6     S3      a  0.00  0.00  0.00  0.00  1.00\n",
      "7     S3      b  0.25  0.25  0.25  0.00  0.25\n",
      "8      F     NA  0.00  0.00  0.00  0.00  1.00 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "line 9:28 extraneous input '1' expecting {<EOF>, ';', '+', ID}\n"
     ]
    }
   ],
   "source": [
    "chemin_plus_court_arriver_en_f = run_mdp(path = \"mdp_examples//chemin_plus_court_arriver_en_f.mdp\", return_printer=True, print_transactions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](chemin_plus_court_arriver_en_f.png)\n",
    "\n",
    "Image générée par l'archive `Plot_colors.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'S0': 1, 'S1': 1, 'S2': 1, 'S3': 1, 'F': 0}\n"
     ]
    }
   ],
   "source": [
    "print(chemin_plus_court_arriver_en_f.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- S segments:\n",
      "S_sure=['F']\n",
      "S_may=['S2', 'S0', 'S1', 'S3']\n",
      "S_never=[]\n",
      "--------- System constraints:\n",
      "Ubfollower=['S2', ['a', 'b'], 'S0', ['a', 'b'], 'S1', ['a', 'b'], 'S3', ['a', 'b']]\n",
      "A_ub=array([[ 1.  , -0.  , -0.  , -1.  ],\n",
      "       [ 1.  , -0.25, -0.25, -0.25],\n",
      "       [-0.  ,  1.  , -1.  , -0.  ],\n",
      "       [-0.25,  1.  , -0.25, -0.25],\n",
      "       [-1.  , -0.  ,  1.  , -0.  ],\n",
      "       [-0.25, -0.25,  1.  , -0.25],\n",
      "       [-0.  , -0.  , -0.  ,  1.  ],\n",
      "       [-0.25, -0.25, -0.25,  1.  ]])\n",
      "b_ub=[1, 1, 1, 1, 1, 1, 1, 1]\n",
      "A_eq=None\n",
      "b_eq=None\n",
      "c=array([-1., -1., -1., -1.])\n",
      "--------- Solution:\n",
      "res.x=array([2.        , 2.33333333, 2.33333333, 1.        ])\n",
      "res.fun=-7.666666666666668\n",
      "res.success=True\n",
      "res.slack=array([0.        , 0.41666667, 1.        , 0.        , 0.66666667,\n",
      "       0.        , 0.        , 1.66666667])\n",
      "res.con=array([], dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "reward_analysis(chemin_plus_court_arriver_en_f, 'F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interprétation du résultat\n",
    "A nouveau, les vecteurs d'intêret sont `res.x` et `res.slack`. \n",
    "\n",
    "A travers `res.x`, nous voyons l'éspérance du nombre de tours pour arriver en $F$ à partir d'un état donnée avec un adversaire qui minimize . Pour l'état $S3$, l'adversaire optimal aura une éspérance 1 par exemple.\n",
    "\n",
    "Quelle stratégie ce adversaire aura? Pour ça, il faut analyser `res.slack` pour savoir quelles contrainter sont actives. Dans notre cas, pour l'état $S3$, l'adversaire optimal est de choisir l'action $a$.\n",
    "\n",
    "Interpretant le résultat pour les autres états, nous avons:\n",
    "\n",
    "$$Adversaire \\space optimal = \\{E_{S_0}(b) = 2.\\overline{3}, E_{S_1}(b) = 2.\\overline{3}, E_{S_2}(a) = 2, E_{S_3}(a) = 1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple d'implémentation de l'algo avec des récompenses dans un cas où elle peut être mal interprété\n",
    "\n",
    "Ici, nous prénons un graphe de simulation de lancer d'un dé a travers des lancés de pièces où toutes les récompenses sont égales à 1.  De cette façon, minimizer les récompenses est equivalent aussi a l'idée de trouver l'éspérance de lancers jusqu'a ce qu'on trouve un numéro. Mais, bien sûr, l'algorithme marche pour des récompenses différentes de 1.\n",
    "\n",
    "Ici, l'erreur viendra du fait que comme on n'arrive pas toujours au sommet, les récompenses attendues sont $+ \\inf$ en réalité, mais l'algorithme ne montre pas ça."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['I', 'S0', 'S1', 'S2', 'S3', 'S4', 'S5', 'F1', 'F2', 'F3', 'F4', 'F5', 'F6']\n",
      "Initialy declared actions: ['NA']\n",
      "Transition from I with no action and targets ['S0', 'S1'] with weights [1, 1]\n",
      "Transition from S0 with no action and targets ['S2', 'S3'] with weights [1, 1]\n",
      "Transition from S1 with no action and targets ['S4', 'S5'] with weights [1, 1]\n",
      "Transition from S2 with no action and targets ['F1', 'S0'] with weights [1, 1]\n",
      "Transition from S3 with no action and targets ['F2', 'F3'] with weights [1, 1]\n",
      "Transition from S4 with no action and targets ['S1', 'F4'] with weights [1, 1]\n",
      "Transition from S5 with no action and targets ['F5', 'F6'] with weights [1, 1]\n",
      "\n",
      " ------- transactions df -------\n",
      "  Origin Action    I   S0   S1   S2   S3   S4   S5   F1   F2   F3   F4   F5  \\\n",
      "0      I     NA  NaN    1    1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "1     S0     NA  NaN  NaN  NaN    1    1  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "2     S1     NA  NaN  NaN  NaN  NaN  NaN    1    1  NaN  NaN  NaN  NaN  NaN   \n",
      "3     S2     NA  NaN    1  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN  NaN  NaN   \n",
      "4     S3     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1    1  NaN  NaN   \n",
      "5     S4     NA  NaN  NaN    1  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1  NaN   \n",
      "6     S5     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1   \n",
      "\n",
      "    F6  \n",
      "0  NaN  \n",
      "1  NaN  \n",
      "2  NaN  \n",
      "3  NaN  \n",
      "4  NaN  \n",
      "5  NaN  \n",
      "6    1  \n",
      "\n",
      " ------- transactions_prob df -------\n",
      "  Origin Action    I   S0   S1   S2   S3   S4   S5   F1   F2   F3   F4   F5  \\\n",
      "0      I     NA  0.0  0.5  0.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "1     S0     NA  0.0  0.0  0.0  0.5  0.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2     S1     NA  0.0  0.0  0.0  0.0  0.0  0.5  0.5  0.0  0.0  0.0  0.0  0.0   \n",
      "3     S2     NA  0.0  0.5  0.0  0.0  0.0  0.0  0.0  0.5  0.0  0.0  0.0  0.0   \n",
      "4     S3     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.5  0.5  0.0  0.0   \n",
      "5     S4     NA  0.0  0.0  0.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.5  0.0   \n",
      "6     S5     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.5   \n",
      "\n",
      "    F6  \n",
      "0  0.0  \n",
      "1  0.0  \n",
      "2  0.0  \n",
      "3  0.0  \n",
      "4  0.0  \n",
      "5  0.0  \n",
      "6  0.5   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chemin_plus_court_lancer_de_pieces = run_mdp(path = \"mdp_examples//chemin_plus_court_lancer_de_pieces.mdp\", return_printer=True, print_transactions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](chemin_plus_court_lancer_de_pieces.png \"MC qui simule un dé à 6 faces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 1, 'S0': 1, 'S1': 1, 'S2': 1, 'S3': 1, 'S4': 1, 'S5': 1, 'F1': 1, 'F2': 1, 'F3': 1, 'F4': 1, 'F5': 1, 'F6': 1}\n"
     ]
    }
   ],
   "source": [
    "print(chemin_plus_court_lancer_de_pieces.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- S segments:\n",
      "S_sure=['F4']\n",
      "S_may=['S4', 'S1', 'I']\n",
      "S_never=['F1', 'F5', 'S0', 'F6', 'F3', 'S3', 'F2', 'S5', 'S2']\n",
      "--------- System constraints:\n",
      "Ubfollower=[]\n",
      "A_ub=None\n",
      "b_ub=None\n",
      "A_eq=array([[ 1. , -0.5, -0. ],\n",
      "       [-0.5,  1. , -0. ],\n",
      "       [-0. , -0.5,  1. ]])\n",
      "b_eq=[1, 1, 1]\n",
      "c=array([-1., -1., -1.])\n",
      "--------- Solution:\n",
      "res.x=array([2., 2., 2.])\n",
      "res.fun=-6.0\n",
      "res.success=True\n",
      "res.slack=array([], dtype=float64)\n",
      "res.con=array([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Analyse pour avoir 4:\n",
    "reward_analysis(chemin_plus_court_lancer_de_pieces, 'F4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interprétation\n",
    "\n",
    "L'interprétation est similaire a celle de l'algorithme précedent. \n",
    "Ici, nous voyons que le nombre de lancers atttendu pour avoir S4 est de 2. \n",
    "\n",
    "Cette valeur, a prémière vue, semble incorrecte. Bien sûr, puisque c'est impossible d'atteindre S4 en 2 lancers, il faut au minimum 3.\n",
    "\n",
    "Pourquoi la fonction retourne ceci? Parce que nous n'avons pas considéré les états $S_{never}$ dans notre solution. L'éspérance associé a eux est de $+\\inf$. Donc, si nous les incluent, la réponse triviale est de $+\\inf$ pour tous les états $S_{may}$. \n",
    "\n",
    "L'intuition est que notre algorithme considère que leur éspérance est de zéro. D'une certaine façon, si nous arrivons dans $S_{never}$, ça ne sert a rien de continuer a lancer des pièces. Donc, nous nous arrrêtons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 15\n",
    "Simulateur de lancers de pièces successifs pour simuler un dé à 6 faces. Cette fois-ci prénant l'exemple utilisée par le professeur lors de la prémière évaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['I', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6']\n",
      "Initialy declared actions: ['a']\n",
      "Transition from I with no action and targets ['T1', 'T2'] with weights [1, 1]\n",
      "Transition from T1 with no action and targets ['T3', 'T4'] with weights [1, 1]\n",
      "Transition from T2 with no action and targets ['T5', 'T6'] with weights [1, 1]\n",
      "Transition from T3 with no action and targets ['S1', 'T1'] with weights [1, 1]\n",
      "Transition from T4 with no action and targets ['S2', 'S3'] with weights [1, 1]\n",
      "Transition from T5 with no action and targets ['S4', 'S5'] with weights [1, 1]\n",
      "Transition from T6 with no action and targets ['S6', 'T2'] with weights [1, 1]\n",
      "Transition from S1 with no action and targets ['S1'] with weights [1]\n",
      "Transition from S2 with no action and targets ['S2'] with weights [1]\n",
      "Transition from S3 with no action and targets ['S3'] with weights [1]\n",
      "Transition from S4 with no action and targets ['S4'] with weights [1]\n",
      "Transition from S5 with no action and targets ['S5'] with weights [1]\n",
      "Transition from S6 with no action and targets ['S6'] with weights [1]\n",
      "\n",
      " ------- transactions df -------\n",
      "  Origin Action    I   T1   T2   T3   T4   T5   T6   S1   S2   S3   S4   S5  \\\n",
      "0      I     NA  NaN    1    1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "1     T1     NA  NaN  NaN  NaN    1    1  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "2     T2     NA  NaN  NaN  NaN  NaN  NaN    1    1  NaN  NaN  NaN  NaN  NaN   \n",
      "3     T3     NA  NaN    1  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN  NaN  NaN   \n",
      "4     T4     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1    1  NaN  NaN   \n",
      "5     T5     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1    1   \n",
      "6     T6     NA  NaN  NaN    1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "7     S1     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN  NaN  NaN   \n",
      "8     S2     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN  NaN   \n",
      "9     S3     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN   \n",
      "\n",
      "    S6  \n",
      "0  NaN  \n",
      "1  NaN  \n",
      "2  NaN  \n",
      "3  NaN  \n",
      "4  NaN  \n",
      "5  NaN  \n",
      "6    1  \n",
      "7  NaN  \n",
      "8  NaN  \n",
      "9  NaN  \n",
      "\n",
      " ------- transactions_prob df -------\n",
      "  Origin Action    I   T1   T2   T3   T4   T5   T6   S1   S2   S3   S4   S5  \\\n",
      "0      I     NA  0.0  0.5  0.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "1     T1     NA  0.0  0.0  0.0  0.5  0.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2     T2     NA  0.0  0.0  0.0  0.0  0.0  0.5  0.5  0.0  0.0  0.0  0.0  0.0   \n",
      "3     T3     NA  0.0  0.5  0.0  0.0  0.0  0.0  0.0  0.5  0.0  0.0  0.0  0.0   \n",
      "4     T4     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.5  0.5  0.0  0.0   \n",
      "5     T5     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.5  0.5   \n",
      "6     T6     NA  0.0  0.0  0.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "7     S1     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
      "8     S2     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
      "9     S3     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
      "\n",
      "    S6  \n",
      "0  0.0  \n",
      "1  0.0  \n",
      "2  0.0  \n",
      "3  0.0  \n",
      "4  0.0  \n",
      "5  0.0  \n",
      "6  0.5  \n",
      "7  0.0  \n",
      "8  0.0  \n",
      "9  0.0   \n",
      "\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - State I reward wasn't assigned, using zero as reward\n",
      "( 1 ) - State T1 reward wasn't assigned, using zero as reward\n",
      "( 2 ) - State T2 reward wasn't assigned, using zero as reward\n",
      "( 3 ) - State T3 reward wasn't assigned, using zero as reward\n",
      "( 4 ) - State T4 reward wasn't assigned, using zero as reward\n",
      "( 5 ) - State T5 reward wasn't assigned, using zero as reward\n",
      "( 6 ) - State T6 reward wasn't assigned, using zero as reward\n",
      "( 7 ) - State S1 reward wasn't assigned, using zero as reward\n",
      "( 8 ) - State S2 reward wasn't assigned, using zero as reward\n",
      "( 9 ) - State S3 reward wasn't assigned, using zero as reward\n",
      "( 10 ) - State S4 reward wasn't assigned, using zero as reward\n",
      "( 11 ) - State S5 reward wasn't assigned, using zero as reward\n",
      "( 12 ) - State S6 reward wasn't assigned, using zero as reward\n"
     ]
    }
   ],
   "source": [
    "de_6_faces = run_mdp(path = \"prof_examples//simu-mc.mdp\", return_printer=True, print_transactions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img_de_6_faces.png \"MC qui simule un dé à 6 faces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction ci-dessous correspond à un générateur d'adversaires qui propose par défaut un choix aléatoire d'actions. Cette fonction nous servira lorsque nous mettrons en œuvre l'algorithme\n",
    "qui traite des processus de décision markoviens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def gerar_preferencias_acoes(df, estados, acoes, modo=\"random\"):\n",
    "    \"\"\"\n",
    "    Generates a dictionary mapping each state to a list of preferred actions.\n",
    "\n",
    "    This function allows for the specification of action preferences for each state\n",
    "    within a Markov Decision Process (MDP), either through direct user input or\n",
    "    by generating a random order of actions.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): A DataFrame containing the transition probabilities for each state-action pair.\n",
    "    - estados (list): A list of all states declared in the MDP.\n",
    "    - acoes (list): A list of all actions declared in the MDP.\n",
    "    - modo (str): The mode of preference generation, either 'input' for user-defined preferences or 'random' for automatically generated preferences.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are states and values are lists of actions, ordered by preference.\n",
    "    \"\"\"\n",
    "    preferencias = {}\n",
    "\n",
    "    for estado in estados:\n",
    "        acoes_possiveis = df[df['Origin'] == estado]['Action'].unique()\n",
    "        \n",
    "        # Filtrar ações possíveis que não sejam 'NA', caso existam\n",
    "        acoes_validas = [acao for acao in acoes_possiveis if acao != \"NA\"]\n",
    "        \n",
    "        if modo == \"input\":\n",
    "            # Se houver apenas uma ação válida ou a ação é 'NA', seleção automática\n",
    "            if len(acoes_validas) <= 1:\n",
    "                preferencias[estado] = acoes_validas if acoes_validas else [\"NA\"]\n",
    "            else:\n",
    "                print(f\"\\nCurrent State: {estado}\")\n",
    "                print(\"Possible Actions: \" + \", \".join(acoes_validas))\n",
    "                preferred = input(f\"Type in the preferred action for {estado} \\n\").strip()\n",
    "                while preferred not in acoes_validas:\n",
    "                    preferred = input(f\"he inputed action {preferred} isn't available for this state, choose one in {acoes_validas} \\n\").strip()\n",
    "                preferencias[estado] = [preferred]\n",
    "\n",
    "        elif modo == \"random\":\n",
    "            random.shuffle(acoes_validas)\n",
    "            preferencias[estado] = acoes_validas\n",
    "            \n",
    "    return preferencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonteCarlo_random_walk(p, target_state, num_transitions = 20):\n",
    "    \"\"\"\n",
    "    Simulates a random walk through an MDP based on action preferences and transition probabilities.\n",
    "\n",
    "    This function simulates traversing through a Markov Decision Process, making decisions at each state\n",
    "    based on predefined or user-specified action preferences. It illustrates the potential path an agent\n",
    "    might take, considering the MDP's transition probabilities for each action.\n",
    "\n",
    "    Parameters:\n",
    "    - p: An object containing the MDP structure, including its states, actions, transition probabilities, and other relevant data.\n",
    "\n",
    "    Returns:\n",
    "    - None: This function does not return a value but prints the simulation results, including the traversed path, actions taken, probabilities of transitions, and the total path probability.\n",
    "    \"\"\"\n",
    "        \n",
    "    df = p.transactions_prob\n",
    "    found = False\n",
    "\n",
    "    preferencias = gerar_preferencias_acoes(df, p.declared_states,p.declared_actions, modo=\"random\")\n",
    "\n",
    "    estado_atual = p.first_state\n",
    "\n",
    "    caminho = [estado_atual]  # Iniciar o registro do caminho com o estado inicial\n",
    "    probabilidade_acumulada = 1\n",
    "\n",
    "    for _ in range(num_transitions):\n",
    "        df_estado_atual = df[df['Origin'] == estado_atual]\n",
    "        if df_estado_atual.empty:\n",
    "            print(\"Stopped at a end of graph state\")\n",
    "            return\n",
    "        acao_selecionada = None\n",
    "        probabilidade_escolhida = None\n",
    "\n",
    "        if df_estado_atual.iloc[0]['Action'] == \"NA\":\n",
    "            probabilidades = df_estado_atual.iloc[0, 2:].astype(float).values\n",
    "            acao_selecionada = \"NA\"\n",
    "        else:\n",
    "            for acao_preferida in preferencias[estado_atual]:\n",
    "                df_acao_preferida = df_estado_atual[df_estado_atual['Action'] == acao_preferida]\n",
    "                if not df_acao_preferida.empty:\n",
    "                    probabilidades = df_acao_preferida.iloc[0, 2:].astype(float).values\n",
    "                    acao_selecionada = acao_preferida\n",
    "                    break\n",
    "\n",
    "        probabilidades = probabilidades / np.sum(probabilidades)\n",
    "        estados_possiveis = df_estado_atual.columns[2:]\n",
    "        proximo_estado = np.random.choice(estados_possiveis, p=probabilidades)\n",
    "        probabilidade_escolhida = probabilidades[np.where(estados_possiveis == proximo_estado)[0][0]]\n",
    "        \n",
    "        probabilidade_acumulada *= probabilidade_escolhida\n",
    "        estado_passado = estado_atual\n",
    "        estado_atual = proximo_estado\n",
    "        caminho.append(estado_atual)  # Atualizar o caminho\n",
    "\n",
    "        if target_state in caminho:\n",
    "            found = True\n",
    "            break\n",
    "\n",
    "\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target state was reached.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "if MonteCarlo_random_walk(de_6_faces, target_state=\"S1\"):\n",
    "    print(\"The target state was reached.\")\n",
    "else:\n",
    "    print(\"The target state was not reached.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 16\n",
    "Algorithme de SMC quantitatif - simulant un dé à 6 faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonteCarloSimulator(p, target_state, num_simulations = 1000, num_transitions = 20):\n",
    "    \"\"\"\n",
    "    Simulates a random walk through an MDP based on action preferences and transition probabilities.\n",
    "\n",
    "    This function simulates traversing through a Markov Decision Process, making decisions at each state\n",
    "    based on predefined or user-specified action preferences. It illustrates the potential path an agent\n",
    "    might take, considering the MDP's transition probabilities for each action.\n",
    "\n",
    "    Parameters:\n",
    "    - p: An object containing the MDP structure, including its states, actions, transition probabilities, and other relevant data.\n",
    "\n",
    "    Returns:\n",
    "    - None: This function does not return a value but prints the simulation results, including the traversed path, actions taken, probabilities of transitions, and the total path probability.\n",
    "    \"\"\"\n",
    "    found = 0\n",
    "    for i in range(num_simulations):\n",
    "        if MonteCarlo_random_walk(p, target_state, num_transitions):\n",
    "            found += 1\n",
    "    prob_reacing = round(found/num_simulations,4)\n",
    "    print(f\"Target state {target_state} was found in {found} out of {num_simulations} simulations. Probability of reaching it: {prob_reacing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target state S1 was found in 16 out of 100 simulations. Probability of reaching it: 0.16\n"
     ]
    }
   ],
   "source": [
    "MonteCarloSimulator(de_6_faces, target_state=\"S1\", num_simulations=100, num_transitions=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 17.1\n",
    "\n",
    "Implémenter l’algorithme de SMC qualitatif utilisant SPRT (Sequential Probability Ratio Test [Wald, 1945]) dans le programme de l’exercice 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SPRT(p, target_state, p0, p1, alpha, beta, num_transitions=20):\n",
    "    \"\"\"\n",
    "    Sequential Probability Ratio Test (SPRT) to determine if the probability of reaching a target state\n",
    "    exceeds a specific threshold, with predefined significance levels and power.\n",
    "    \n",
    "    Parameters:\n",
    "    - p: MDP or MC structure.\n",
    "    - target_state: The target state we want to test for.\n",
    "    - p0: The probability of success under the null hypothesis H0.\n",
    "    - p1: The probability of success under the alternative hypothesis H1.\n",
    "    - alpha: The significance level (probability of rejecting H0 when H0 is true).\n",
    "    - beta: The test power (1 - probability of accepting H0 when H1 is true).\n",
    "    - num_transitions: Maximum number of transitions per simulation.\n",
    "    \n",
    "    Returns:\n",
    "    - A string indicating the test outcome (accept H0, accept H1, or continue data collection).\n",
    "    \"\"\"\n",
    "    log_lambda = 0  # Log likelihood ratio\n",
    "    A = np.log((1 - beta) / alpha)  # Threshold to accept H1\n",
    "    B = np.log(beta / (1 - alpha))  # Threshold to accept H0\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        result = MonteCarlo_random_walk(p, target_state, num_transitions)\n",
    "        \n",
    "        # Calculate the log likelihood ratio based on the result\n",
    "        if result:\n",
    "            log_likelihood_ratio = np.log(p1 / p0)\n",
    "        else:\n",
    "            log_likelihood_ratio = np.log((1 - p1) / (1 - p0))\n",
    "        \n",
    "        log_lambda += log_likelihood_ratio\n",
    "\n",
    "        if log_lambda >= A:\n",
    "            return \"Accept H1: The probability of reaching the target state is significantly high.\"\n",
    "        elif log_lambda <= B:\n",
    "            return \"Accept H0: The probability of reaching the target state is not significantly high.\"\n",
    "        # Otherwise, continue collecting data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ##### \"Accepter H1 : La probabilité d'atteindre l'état cible est significativement élevée.\"\n",
    "\n",
    "Signification : Le test a déterminé, avec le niveau de confiance spécifié par $1 - \\alpha$, que la probabilité d'atteindre l'état cible est significativement plus élevée que la probabilité sous l'hypothèse nulle $p_0$. Cela signifie que, sur la base des simulations effectuées, il existe des preuves suffisantes pour rejeter l'hypothèse nulle en faveur de l'hypothèse alternative. En d'autres termes, il est très probable que la véritable probabilité d'atteindre l'état cible soit supérieure à $p_0$, jusqu'au seuil défini par $p_1$.\n",
    "\n",
    "- #### \"Accepter H0 : La probabilité d'atteindre l'état cible n'est pas significativement élevée.\"\n",
    "\n",
    "Signification : Le test a conclu, avec le niveau de confiance spécifié par $\\beta$, qu'il n'y a pas de preuves suffisantes pour affirmer que la probabilité d'atteindre l'état cible est supérieure à la probabilité sous l'hypothèse alternative $p_1$. Par conséquent, l'hypothèse nulle n'est pas rejetée. Cela ne signifie pas nécessairement que l'hypothèse nulle est vraie, mais que, sur la base des simulations réalisées, il n'a pas été possible de démontrer que la probabilité de succès est significativement élevée comme défini par $p_1$. On peut dire que, dans le niveau de signification défini, la probabilité d'atteindre l'état cible ressemble davantage à $p_0$ ou est inférieure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accept H1: The probability of reaching the target state is significantly high.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPRT(de_6_faces, target_state=\"S1\", p0=0.1, p1=0.15, alpha=0.01, beta=0.01, num_transitions=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 17.2\n",
    "On appelle $p_{ki}$ la probabilité d’obtenir le chiffre $i$ après $k$ lancers au maximum. Utiliser votre programme pour estimer $P(p_{ki} \\geq 0.1)$, $P(p_{ki} \\geq 0.14)$, $P(p_{ki} \\geq 0.16)$. Qu’observez-vous ? Utiliser $\\alpha = \\beta = 0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def estima_probabilites(p, target_state, seuils, alpha, beta, num_transitions=20, num_simulations=1000):\n",
    "    \"\"\"\n",
    "    Estime les probabilités P(p_{ki} >= seuil) pour chaque seuil spécifié.\n",
    "    \n",
    "    Paramètres :\n",
    "    - p : La structure MDP ou MC.\n",
    "    - target_state : L'état cible à tester.\n",
    "    - seuils : Une liste des seuils (p0) à tester.\n",
    "    - alpha : Le niveau de signification pour rejeter H0.\n",
    "    - beta : La puissance du test (1 - probabilité d'accepter H0 quand H1 est vrai).\n",
    "    - num_transitions : Nombre maximum de transitions par simulation.\n",
    "    - num_simulations : Nombre de simulations à exécuter pour chaque seuil.\n",
    "    \n",
    "    Retourne :\n",
    "    - Un dictionnaire avec les seuils comme clés et les probabilités estimées comme valeurs.\n",
    "    \"\"\"\n",
    "    resultats = {seuil: 0 for seuil in seuils}\n",
    "    p1 = max(seuils) + 0.05  # Définir p1 comme une valeur un peu plus élevée que le seuil le plus élevé\n",
    "\n",
    "    for seuil in seuils:\n",
    "        accepte_h1 = 0\n",
    "        \n",
    "        for _ in tqdm(range(num_simulations), desc=f\"Seuil {seuil}\"):\n",
    "            decision = SPRT(p, target_state, seuil, p1, alpha, beta, num_transitions)\n",
    "            if decision == \"Accept H1: The probability of reaching the target state is significantly high.\":\n",
    "                accepte_h1 += 1\n",
    "        \n",
    "        probabilite_estimee = accepte_h1 / num_simulations\n",
    "        resultats[seuil] = probabilite_estimee\n",
    "        \n",
    "    return resultats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seuil 0.1:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seuil 0.1: 100%|██████████| 10/10 [01:17<00:00,  7.73s/it]\n",
      "Seuil 0.14: 100%|██████████| 10/10 [03:18<00:00, 19.88s/it]\n",
      "Seuil 0.16: 100%|██████████| 10/10 [03:41<00:00, 22.15s/it]\n",
      "Seuil 0.17: 100%|██████████| 10/10 [02:36<00:00, 15.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.1: 0.8, 0.14: 0.2, 0.16: 0.0, 0.17: 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "p = de_6_faces\n",
    "target_state = \"S1\"\n",
    "seuils = [0.1, 0.14, 0.16, 0.17]\n",
    "alpha = 0.01\n",
    "beta = 0.01\n",
    "num_transitions = 10\n",
    "num_simulations = 10\n",
    "\n",
    "resultats = estima_probabilites(p, target_state, seuils, alpha, beta, num_transitions, num_simulations)\n",
    "print(resultats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 17.3\n",
    "Tester sur le modèle du CRAPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['I', 'P410', 'P59', 'P68', 'P', 'G']\n",
      "Initialy declared actions: ['none']\n",
      "Transition from I with no action and targets ['P', 'P410', 'P59', 'P68', 'G'] with weights [2, 3, 4, 5, 4]\n",
      "Transition from P410 with no action and targets ['P', 'P410', 'G'] with weights [2, 9, 1]\n",
      "Transition from P59 with no action and targets ['P', 'P59', 'G'] with weights [3, 13, 2]\n",
      "Transition from P68 with no action and targets ['P', 'P68', 'G'] with weights [6, 25, 5]\n",
      "Transition from G with no action and targets ['G'] with weights [1]\n",
      "Transition from P with no action and targets ['P'] with weights [1]\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - State I reward wasn't assigned, using zero as reward\n",
      "( 1 ) - State P410 reward wasn't assigned, using zero as reward\n",
      "( 2 ) - State P59 reward wasn't assigned, using zero as reward\n",
      "( 3 ) - State P68 reward wasn't assigned, using zero as reward\n",
      "( 4 ) - State P reward wasn't assigned, using zero as reward\n",
      "( 5 ) - State G reward wasn't assigned, using zero as reward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "line 1:9 mismatched input ':' expecting {';', ','}\n"
     ]
    }
   ],
   "source": [
    "craps = run_mdp(path = \"mdp_examples//craps.mdp\", return_printer=True, print_transactions=False) # Exemple sans récompenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](craps.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accept H1: The probability of reaching the target state is significantly high.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPRT(craps, target_state=\"G\", p0=0.1, p1=0.15, alpha=0.01, beta=0.01, num_transitions=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seuil 0.1:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seuil 0.1: 100%|██████████| 10/10 [00:02<00:00,  3.39it/s]\n",
      "Seuil 0.14: 100%|██████████| 10/10 [00:04<00:00,  2.21it/s]\n",
      "Seuil 0.16: 100%|██████████| 10/10 [00:07<00:00,  1.30it/s]\n",
      "Seuil 0.17: 100%|██████████| 10/10 [00:09<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.1: 1.0, 0.14: 1.0, 0.16: 1.0, 0.17: 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "p = craps\n",
    "target_state = \"G\"\n",
    "seuils = [0.1, 0.14, 0.16, 0.17]\n",
    "alpha = 0.01\n",
    "beta = 0.01\n",
    "num_transitions = 10\n",
    "num_simulations = 10\n",
    "\n",
    "resultats = estima_probabilites(p, target_state, seuils, alpha, beta, num_transitions, num_simulations)\n",
    "print(resultats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 17.4\n",
    "Que faire si le modèle est un MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre code est suffisamment robuste pour traiter un MDP, puisque l'utilisation de Monte Carlo RandomWalk présuppose déjà le choix d'un adversaire aléatoire.\n",
    "\n",
    "Il nous suffit d'instancier un MDP et d'appeler la fonction en lui passant un argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modélisation Probabiliste et Apprentissage par Renforcement (Chapitre 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous voulons implémenteer un algorithme simple de **Q-learning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, n_states, n_actions, learning_rate=0.1, discount_factor=0.9, epsilon=0.1):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    def choose_action(self, state, possible_actions):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            # Explore: choose a random action\n",
    "            action = np.random.choice(possible_actions)\n",
    "        else:\n",
    "            # Exploit: choose the action with the highest Q-value for the current state\n",
    "            action = np.argmax(self.Q[state])\n",
    "        return action\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        # Q-learning update\n",
    "        best_next_action = np.argmax(self.Q[next_state])\n",
    "        td_target = reward + self.discount_factor * self.Q[next_state, best_next_action]\n",
    "        td_delta = td_target - self.Q[state, action]\n",
    "        self.Q[state, action] += self.learning_rate * td_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pour tester (choisir un état initial et un mdp dans le format printer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actions_of(printer, state):\n",
    "    return printer.transactions_prob.loc[printer.transactions_prob['Origin'] == state, 'Action'].unique()\n",
    "\n",
    "def apply_Q_learning(printer):\n",
    "    n_states = len(printer.declared_states)\n",
    "    n_actions =  len(printer.declared_actions)\n",
    "\n",
    "    # Initialize the Q-learning agent\n",
    "    agent = QLearningAgent(n_states, n_actions)\n",
    "\n",
    "    # Training the agent\n",
    "    n_episodes = 1000\n",
    "    for episode in range(n_episodes):\n",
    "        state = 0  # Initial state\n",
    "        done = False\n",
    "        while not done:\n",
    "            print(printer.declared_actions)\n",
    "            print(state)\n",
    "            action = agent.choose_action(state, actions_of(printer,state))\n",
    "            print(action)\n",
    "            df = printer.transactions_prob\n",
    "            df1 = df.loc[(df['Action']==action) & (df['Origin']==state), [col for col in df.columns if col not in ['Action', 'Origin']]]\n",
    "            df1v = df1.values\n",
    "            df1c = df1.columns\n",
    "            next_state = np.random.choice(len(df1c), p=df1v.ravel())\n",
    "            reward = printer.rewards[state]\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            if state == 2:\n",
    "                done = True\n",
    "                \n",
    "    # Print the learned Q-values\n",
    "    print(\"Learned Q-values:\")\n",
    "    print(agent.Q)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_10_infoia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
