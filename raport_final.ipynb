{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPAR - Rapport du rendu II\n",
    "**01/04/24**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Ce notebook présente l'explication, la démonstration et les résultats des outils développés dans le cadre du cours de MPAR. Les résultats de la première partie du rendu, discutés en cours, ne seront pas abordés ici mais seront uniquement utilisés comme référence.\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "1. Comprendre les concepts de **Model Checking** et de **SMC (Statistical Model Checking)** pour les **chaînes de Markov**.\n",
    "2. Étendre ces concepts aux **MDP (Markov Decision Process)**.\n",
    "3. Utiliser le **RL (Reinforcement Learning)** pour améliorer le model checking statistique.\n",
    "\n",
    "## Contenu\n",
    "1. **Changements dans le parser**\n",
    "2. **Chapitre 2, Vérification Probabiliste**\n",
    "3. **Chapitre 3, Modélisation Probabiliste et Apprentissage par Renforcement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Changements dans le parser\n",
    "\n",
    "Pour pouvoir utiliser des modèles .mdp avec des recompenses, il a fallut modifier notre parser. Pour ceci, les anciens modèles .mdp, sans récompenses, doivent continuer a fonctionnner sans altération.\n",
    "\n",
    "De cette façon, nous avons inclu la ligne optionnelle `Rewards` avec des entrées dans le format `s:r` avec s un état et r une récompense entière. L'exemple `states_with_rewards` montre ceci.\n",
    "\n",
    "Ceci a été fait en ajoutant l'entrée (defrewards) optionnele (?)  a la définition du programme, ayant `program: defstates defrewards? defactions transitions EOF;` et ayant la définition `defewrards : REWARDS ID ':' INT (',' ID ':' INT)* ';';`.\n",
    "\n",
    "Après, nous avons ajouté l'attribut `rewards` à la classe `gramPrintListener`, défini a travers les fonctions `enterDefrewards` et `update_rewards`.\n",
    "\n",
    "De cette façon, comme dans l'exemple vu en cours, il suffit d'importer la fonction `run` de l'archive `mdp.py` pour lire un fichier et creer un objet de la classe `gramPrintListener` qui contiendrat, en plus des attributs vus dans la prémière démostration, les attributs de récompenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp import run as run_mdp # Fonction qui crée l'objet a être lu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple sans récompenses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['I', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6']\n",
      "Initialy declared actions: ['a']\n",
      "Transition from I with no action and targets ['T1', 'T2'] with weights [1, 1]\n",
      "Transition from T1 with no action and targets ['T3', 'T4'] with weights [1, 1]\n",
      "Transition from T2 with no action and targets ['T5', 'T6'] with weights [1, 1]\n",
      "Transition from T3 with no action and targets ['S1', 'T1'] with weights [1, 1]\n",
      "Transition from T4 with no action and targets ['S2', 'S3'] with weights [1, 1]\n",
      "Transition from T5 with no action and targets ['S4', 'S5'] with weights [1, 1]\n",
      "Transition from T6 with no action and targets ['S6', 'T2'] with weights [1, 1]\n",
      "Transition from S1 with no action and targets ['S1'] with weights [1]\n",
      "Transition from S2 with no action and targets ['S2'] with weights [1]\n",
      "Transition from S3 with no action and targets ['S3'] with weights [1]\n",
      "Transition from S4 with no action and targets ['S4'] with weights [1]\n",
      "Transition from S5 with no action and targets ['S5'] with weights [1]\n",
      "Transition from S6 with no action and targets ['S6'] with weights [1]\n",
      "\n",
      " ------- transactions df -------\n",
      "  Origin Action    I   T1   T2   T3   T4   T5   T6   S1   S2   S3   S4   S5  \\\n",
      "0      I     NA  NaN    1    1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "1     T1     NA  NaN  NaN  NaN    1    1  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "2     T2     NA  NaN  NaN  NaN  NaN  NaN    1    1  NaN  NaN  NaN  NaN  NaN   \n",
      "3     T3     NA  NaN    1  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN  NaN  NaN   \n",
      "4     T4     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1    1  NaN  NaN   \n",
      "5     T5     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1    1   \n",
      "6     T6     NA  NaN  NaN    1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "7     S1     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN  NaN  NaN   \n",
      "8     S2     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN  NaN   \n",
      "9     S3     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN   \n",
      "\n",
      "    S6  \n",
      "0  NaN  \n",
      "1  NaN  \n",
      "2  NaN  \n",
      "3  NaN  \n",
      "4  NaN  \n",
      "5  NaN  \n",
      "6    1  \n",
      "7  NaN  \n",
      "8  NaN  \n",
      "9  NaN  \n",
      "\n",
      " ------- transactions_prob df -------\n",
      "  Origin Action    I   T1   T2   T3   T4   T5   T6   S1   S2   S3   S4   S5  \\\n",
      "0      I     NA  0.0  0.5  0.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "1     T1     NA  0.0  0.0  0.0  0.5  0.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2     T2     NA  0.0  0.0  0.0  0.0  0.0  0.5  0.5  0.0  0.0  0.0  0.0  0.0   \n",
      "3     T3     NA  0.0  0.5  0.0  0.0  0.0  0.0  0.0  0.5  0.0  0.0  0.0  0.0   \n",
      "4     T4     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.5  0.5  0.0  0.0   \n",
      "5     T5     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.5  0.5   \n",
      "6     T6     NA  0.0  0.0  0.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "7     S1     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
      "8     S2     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
      "9     S3     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
      "\n",
      "    S6  \n",
      "0  0.0  \n",
      "1  0.0  \n",
      "2  0.0  \n",
      "3  0.0  \n",
      "4  0.0  \n",
      "5  0.0  \n",
      "6  0.5  \n",
      "7  0.0  \n",
      "8  0.0  \n",
      "9  0.0   \n",
      "\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - State I reward wasn't assigned, using zero as reward\n",
      "( 1 ) - State T1 reward wasn't assigned, using zero as reward\n",
      "( 2 ) - State T2 reward wasn't assigned, using zero as reward\n",
      "( 3 ) - State T3 reward wasn't assigned, using zero as reward\n",
      "( 4 ) - State T4 reward wasn't assigned, using zero as reward\n",
      "( 5 ) - State T5 reward wasn't assigned, using zero as reward\n",
      "( 6 ) - State T6 reward wasn't assigned, using zero as reward\n",
      "( 7 ) - State S1 reward wasn't assigned, using zero as reward\n",
      "( 8 ) - State S2 reward wasn't assigned, using zero as reward\n",
      "( 9 ) - State S3 reward wasn't assigned, using zero as reward\n",
      "( 10 ) - State S4 reward wasn't assigned, using zero as reward\n",
      "( 11 ) - State S5 reward wasn't assigned, using zero as reward\n",
      "( 12 ) - State S6 reward wasn't assigned, using zero as reward\n"
     ]
    }
   ],
   "source": [
    "simu_mc = run_mdp(path = \"prof_examples//simu-mc.mdp\", return_printer=True, print_transactions=True) # Exemple sans récompenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple avec récompenses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['S0', 'S1', 'S2']\n",
      "Initialy declared actions: ['a', 'b', 'c']\n",
      "Transition from S0 with no action and targets ['S1', 'S2'] with weights [5, 5]\n",
      "Transition from S1 with action b and targets ['S1', 'S0'] with weights [2, 8]\n",
      "Transition from S1 with action a and targets ['S2', 'S0', 'S1', 'S3'] with weights [1, 3, 6, 2]\n",
      "Transition from S2 with action c and targets ['S0', 'S1', 'S3'] with weights [5, 5, 10]\n",
      "Transition from S2 with action d and targets ['S0', 'S3'] with weights [5, 7]\n",
      "Transition from S3 with action e and targets ['S1', 'S2'] with weights [2, 2]\n",
      "\n",
      " ------- transactions df -------\n",
      "  Origin Action   S0   S1   S2    S3\n",
      "0     S0     NA  NaN    5    5   NaN\n",
      "1     S1      b    8    2  NaN   NaN\n",
      "2     S1      a    3    6    1   2.0\n",
      "3     S2      c    5    5  NaN  10.0\n",
      "4     S2      d    5  NaN  NaN   7.0\n",
      "5     S3      e  NaN    2    2   NaN\n",
      "\n",
      " ------- transactions_prob df -------\n",
      "  Origin Action        S0    S1        S2        S3\n",
      "0     S0     NA  0.000000  0.50  0.500000  0.000000\n",
      "1     S1      b  0.800000  0.20  0.000000  0.000000\n",
      "2     S1      a  0.250000  0.50  0.083333  0.166667\n",
      "3     S2      c  0.250000  0.25  0.000000  0.500000\n",
      "4     S2      d  0.416667  0.00  0.000000  0.583333\n",
      "5     S3      e  0.000000  0.50  0.500000  0.000000 \n",
      "\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - Undeclared state S3 targeted in transition: S1 with action a, declared automaticaly\n",
      "( 1 ) - Undeclared action in transition: S2 with action d, declared automaticaly\n",
      "( 2 ) - Undeclared action in transition: S3 with action e, declared automaticaly\n",
      "( 3 ) - State S3 reward wasn't assigned, using zero as reward\n"
     ]
    }
   ],
   "source": [
    "states_with_rewards = run_mdp(path = \"mdp_examples//states_with_rewards.mdp\", return_printer=True, print_transactions=True) # Exemple avec récompenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme montré dans les exemples précedents, à chaque état nous avons un reward attribué. \n",
    "\n",
    "Si la récompense n'a pas été définie, notre parser attribut une valeur de zero. \n",
    "\n",
    "Il est un dictionnaire et peut être appelé facilement par l'attribut `rewards` comme suit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states_with_reards.mdp {'S0': 1, 'S1': 10, 'S2': 15, 'S3': 0}\n",
      "simu_mc.mdp {'I': 0, 'T1': 0, 'T2': 0, 'T3': 0, 'T4': 0, 'T5': 0, 'T6': 0, 'S1': 0, 'S2': 0, 'S3': 0, 'S4': 0, 'S5': 0, 'S6': 0}\n"
     ]
    }
   ],
   "source": [
    "print('states_with_reards.mdp', states_with_rewards.rewards)\n",
    "print('simu_mc.mdp', simu_mc.rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vérification Probabiliste (Chapitre 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 12: \n",
    "Identification des ensembles $S_0$, $S_1$ et $S_?$ pour une propriété $P(\\diamond s)$ (fatalment $s$, où $s$ est un état)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "Nous avons décidé d'utiliser notre dataframe de probabilitées pour raisoner. L'idée geral de notre algorithme est:\n",
    "1. Au début, aucun état ne peut arriver ($S_{0} = all states $). Nous ne deplaçons les états seulement depuis $S_0$ vers les autres ensembles, jamais le chemin inverse où de $S_?$ vers $S_1$ où vice versa.\n",
    "2. L'état objectif satisfait toujours la propriété, alors on le déplace de $S_{0}$ pour $S_1$\n",
    "3. Maitenant, nous voulons trouver tous les états $S_1$. Pour ça, il faut faire une boucle.\n",
    "    - Nous calculons la probabilité totale de chaque couple (état, action) d'arriver aux états de $S_1$. \n",
    "    - Pour chaque état, entre toutes les actions possibles qu'il peut avoir, si la probabilité totale minimum est de 1, alors toutes sont égals a 1, donc il est déplacé pour $S_1$.\n",
    "    - Si $S_1$ a changé, nous recommençons le calcul. \n",
    "4. Maitenant, pour trouver les états $S_?$, nous refaisons une boucle.\n",
    "    - Idem pour la probabilité d'arrivé, mais cette fois-ci en $S_1$ où $S_?$.\n",
    "    - Ici, nous voulons l'ajouter a $S_?$ si au moins l'une de ces actions peut, peut-être, l'emmener a $S_?$ où $S_?$. Donc, il faut que la probabilité maximum d'un état soit plus grande que zéro.\n",
    "    - Si $S_?$ a changé, nous recommençons le calcul. \n",
    "\n",
    "\n",
    "#### Idée de l'algo\n",
    "\n",
    "L'idée de l'algorithme vient de la façon comme nous analisons un graphe pour trouver les ensembles. D'abord, nous començons toujours par $S_1$ et puis après par $S_?$. Aussi, a chaue fois nous regardons le graphe comme une seule chose au lieu de s'imaginer en marchant sur le graphe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_suremaynever_states(printer, target_state):\n",
    "    # Initialize sets for S_sure, S_may, and S_never\n",
    "    s_sure = set()\n",
    "    s_may = set()\n",
    "    s_never = set(printer.declared_states)\n",
    "    s_sure.add(target_state)\n",
    "    s_never.remove(target_state)\n",
    "    \n",
    "    stop = False\n",
    "    # First cycle to add s_sures\n",
    "    while not stop:\n",
    "        stop = True\n",
    "        probs = printer.transactions_prob.loc[:, ['Origin', 'Action']+list(s_sure)]\n",
    "        probs['P_sure'] = probs.loc[:, list(s_sure)].sum(axis=1)\n",
    "        for o in probs['Origin'].unique():\n",
    "            if probs.loc[probs['Origin']==o]['P_sure'].min() == 1: # It's sure if all actions lead to 100% prob of arriving to sure. \n",
    "                    if o in s_never:                               # Si un nouveau état est ajouté, on continue le cicle\n",
    "                        s_never.remove(o)\n",
    "                        s_sure.add(o)\n",
    "                        stop = False\n",
    "    stop = False\n",
    "    # First cycle to add s_sures\n",
    "    while not stop:\n",
    "        stop = True\n",
    "        probs = printer.transactions_prob.loc[:, ['Origin', 'Action']+list(s_may)+list(s_sure)]\n",
    "        probs['P_may'] = probs.loc[:, list(s_sure)+list(s_may)].sum(axis=1)\n",
    "        for o in probs['Origin'].unique():\n",
    "            if probs.loc[probs['Origin']==o]['P_may'].max() > 0: # It may arrive if at least one action have a probability of arriving to a may or sure. \n",
    "                    if o in s_never:                             # Si un nouveau état est ajouté, on continue le cicle\n",
    "                        s_never.remove(o)\n",
    "                        s_may.add(o)\n",
    "                        stop = False\n",
    "    \n",
    "    return list(s_sure), list(s_may), list(s_never)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['S0', 'S1', 'S2', 'S3']\n",
      "Initialy declared actions: ['a', 'b', 'c']\n",
      "Transition from S0 with action a and targets ['S1', 'S2'] with weights [1, 1]\n",
      "Transition from S0 with action b and targets ['S0'] with weights [1]\n",
      "Transition from S11 with no action and targets ['S2'] with weights [1, 1]\n",
      "Transition from S2 with no action and targets ['S3'] with weights [1]\n",
      "Transition from S1 with no action and targets ['S3'] with weights [1]\n",
      "Transition from S5 with no action and targets ['S6'] with weights [1]\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - Undeclared state in transition: S11, declared automaticaly\n",
      "( 1 ) - Undeclared state in transition: S5, declared automaticaly\n",
      "( 2 ) - Undeclared state S6 targeted in transition from S5 with NA, declared automaticaly\n",
      "( 3 ) - State S0 reward wasn't assigned, using zero as reward\n",
      "( 4 ) - State S1 reward wasn't assigned, using zero as reward\n",
      "( 5 ) - State S2 reward wasn't assigned, using zero as reward\n",
      "( 6 ) - State S3 reward wasn't assigned, using zero as reward\n",
      "( 7 ) - State S11 reward wasn't assigned, using zero as reward\n",
      "( 8 ) - State S5 reward wasn't assigned, using zero as reward\n",
      "( 9 ) - State S6 reward wasn't assigned, using zero as reward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "line 5:11 mismatched input ',' expecting {';', '+'}\n"
     ]
    }
   ],
   "source": [
    "smaysurenever = run_mdp(path = \"mdp_examples//smaysurenever.mdp\", return_printer=True, print_transactions=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['S2', 'S1', 'S11', 'S3'], ['S0'], ['S5', 'S6'])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_sure, S_may, S_never = segment_suremaynever_states(smaysurenever, 'S3')\n",
    "S_sure, sorted(S_may), sorted(S_never)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 13:\n",
    "\n",
    "A partir d'un MDP $ M = (S, \\text{Act}, P, \\iota_{\\text{init}}, AP, L) $ et une propriété $ P_{\\text{max}} (\\diamond s^*) $ avec $ s^* \\in S $ un état donné. Proposer une définition pour la matrice $ A $ et le vecteur $ b $ du programme linéaire $ A \\cdot x \\geq b $. Calculer $x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "Ici, nous voulons une solution gérale, qui marche pour des MDP mais aussi pour des MC. Pour ça, cest important de réetablire les vecteurs et matrices d'inéquations et équations a `None` quand nous n'avons pas d'équation où inéquation.\n",
    "\n",
    "Pour résoudre le problème de programmation linéaire décrit, nous utilisons la bibliothèque `scipy.optimize` avec la fonction `linprog`, visant à minimiser la fonction objectif, qui est la somme des valeurs des variables d'état. L'implémentation de l'algorithme suit plusieurs étapes clés pour définir les contraintes et formuler le problème de manière à pouvoir être résolu efficacement.\n",
    "\n",
    "- **Définition des Matrices et Vecteurs :**\n",
    "  - `A_eq` et `b_eq` : Ces matrices et vecteurs sont utilisés pour représenter les contraintes d'égalité dans le problème de programmation linéaire. Dans le contexte de MDPs ou MCs, ces contraintes correspondent aux transitions d'état où il y a une certitude sur le résultat de l'action, c'est-à-dire lorsque de l'état source, il existe une unique transition possible. Ces contraintes garantissent que la somme des probabilités des transitions pour ces états source spécifiques est égale à un vecteur déterminé par la somme des probabilités vers les états `S_sure`.\n",
    "  - `A_ub` et `b_ub` : Représentent les contraintes d'inégalité. Pour les états source avec plusieurs transitions possibles (dans `S_may`), ces contraintes sont définies pour refléter que la somme des probabilités de transition vers les états `S_may`, moins la probabilité de rester dans le même état, doit être inférieure ou égale aux probabilités de transition vers les états `S_sure`. Cela permet de gérer les incertitudes dans les transitions.\n",
    "\n",
    "- **Logique et Principe de l'Algorithme :**\n",
    "  L'algorithme parcourt tous les états source dans `S_may`. Pour chaque état, selon qu'il y ait une unique transition ou plusieurs, il applique une logique différente :\n",
    "  - Si une unique transition est possible, la contrainte est ajoutée comme une égalité.\n",
    "  - Si plusieurs transitions sont possibles, des contraintes d'inégalité sont ajoutées pour chaque transition possible.\n",
    "\n",
    "Cela est suivi par la définition de la fonction objectif `c`, où nous cherchons à minimiser la somme des variables d'état (représentant les probabilités). \n",
    "\n",
    "Ensuite, les matrices et vecteurs de contraintes sont préparés pour s'assurer qu'ils sont dans le bon format pour `linprog`, et le problème de programmation linéaire est résolu en appelant `linprog` avec les paramètres définis.\n",
    "\n",
    "Le résultat de `linprog` donne la distribution de probabilité optimale des états sous les contraintes données, minimisant ainsi la fonction objectif tout en satisfaisant les contraintes de transition entre les états.\n",
    "\n",
    "Cette approche fournit une méthode systématique et efficace pour résoudre des problèmes complexes de décision stochastique en utilisant la programmation linéaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "def solve_system(printer, S_may, S_sure):\n",
    "    '''\n",
    "    Résout un système de programmation linéaire défini par des contraintes sur les probabilités de transition entre les états d'un modèle de décision de Markov (MDP) ou d'une chaîne de Markov (MC).\n",
    "\n",
    "    Parameters:\n",
    "        printer (Printer): Un objet contenant les probabilités de transactions entre les états sous forme de DataFrame pandas.\n",
    "        S_may (list): Liste des états où il y a potentiellement plusieurs transitions possibles.\n",
    "        S_sure (list): Liste des états où une transition unique est sûre.\n",
    "\n",
    "    Returns:\n",
    "        res: Un objet OptimizeResult contenant le résultat de la résolution du problème de programmation linéaire.\n",
    "        Ce résultat inclut la distribution optimale des probabilités d'état, la valeur de la fonction objectif à l'optimisation, un booléen indiquant si l'optimisation a réussi,\n",
    "        et d'autres informations pertinentes sur le processus d'optimisation.\n",
    "\n",
    "    Cette fonction établit et résout un problème de programmation linéaire pour trouver la distribution optimale des probabilités d'état qui minimisent la somme des probabilités dans les états incertains (S_may),\n",
    "    tout en respectant les contraintes définies par les transitions sûres (S_sure) et les transitions potentielles (S_may) dans les modèles donnés.\n",
    "\n",
    "    Les contraintes d'égalité sont utilisées pour les états avec une transition unique et sûre, tandis que les contraintes d'inégalité sont appliquées aux états avec plusieurs transitions potentielles.\n",
    "    La fonction objectif vise à minimiser la somme des probabilités d'état dans S_may.\n",
    "\n",
    "    Exemple d'utilisation:\n",
    "        printer = Printer()  # Supposons que Printer est une classe définie ailleurs avec l'attribut transactions_prob.\n",
    "        S_may = ['état1', 'état2']\n",
    "        S_sure = ['état3', 'état4']\n",
    "        resultat = solve_system(printer, S_may, S_sure)\n",
    "        print(resultat)\n",
    "    '''\n",
    "     \n",
    "    df = printer.transactions_prob\n",
    "    # Initialize lists for inequality and equality constraints\n",
    "    A_ub, b_ub, A_eq, b_eq = [], [], [], []\n",
    "    Ubfollower = []\n",
    "    # Iterate over source states in S_may\n",
    "    for source_state in S_may:\n",
    "        # Check if there is only one transition from the source state\n",
    "        if len(df.loc[df['Origin'] == source_state]) == 1:\n",
    "            # If only one transition, add it as an equality constraint\n",
    "            t = df.loc[df['Origin'] == source_state, S_may].copy()\n",
    "            t.loc[:, t.columns == source_state] -= 1  # Identity matrix substracted (A-I), but we invert later\n",
    "            A_eq.append(-t.values)                    # (A-I) becomes (I-A)\n",
    "            b_eq.append(np.sum(df.loc[df['Origin'] == source_state, S_sure].values, axis=1))\n",
    "        else:\n",
    "            # If multiple transitions, add them as inequality constraints\n",
    "            mask_state = df['Origin'] == source_state\n",
    "            Ubfollower.append(source_state)\n",
    "            ts = df.loc[mask_state, S_may].copy()\n",
    "            ts.loc[:, ts.columns == source_state] -=1\n",
    "            Ubfollower.append(list(df.loc[mask_state, 'Action']))\n",
    "            for i in range(len(df.loc[mask_state])):\n",
    "                A_ub.append(ts.values[i])\n",
    "                b_ub.append(-np.sum(df.loc[df['Origin'] == source_state, S_sure].values, axis=1)[i])\n",
    "    \n",
    "    c = np.ones(len(S_may)) # Objectif: Minimizer la somme des x de chaque état\n",
    "        \n",
    "    A_ub, b_ub, A_eq, b_eq = [None if not v else v for v in [A_ub, b_ub, A_eq, b_eq]]\n",
    "    A_eq, A_ub = [np.vstack(m) if m is not None else None for m in [A_eq, A_ub]]\n",
    "\n",
    "    # Solve the linear programming problem\n",
    "    print('--------- System constraints:')\n",
    "    print(f\"{Ubfollower=}\")\n",
    "    print(f\"{A_ub=}\")\n",
    "    print(f\"{b_ub=}\")\n",
    "    print(f\"{A_eq=}\")\n",
    "    print(f\"{b_eq=}\")\n",
    "    print(f\"{c=}\")\n",
    "\n",
    "    res = linprog(c=c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=(0,1))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abc_analysis(printer, state):\n",
    "    S_sure, S_may, S_never = segment_suremaynever_states(printer, state)\n",
    "    S_sure, sorted(S_may), sorted(S_never)\n",
    "\n",
    "    print('--------- S segments:')\n",
    "    print(f'{S_sure=}')\n",
    "    print(f'{S_may=}')\n",
    "    print(f'{S_never=}')\n",
    "\n",
    "    res = solve_system(printer, S_may, S_sure)\n",
    "\n",
    "    print('--------- Solution:')\n",
    "    print(f'{res.x=}')\n",
    "    print(f'{res.fun=}')\n",
    "    print(f'{res.success=}')\n",
    "    print(f'{res.slack=}')\n",
    "    print(f'{res.con=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testant pour une MC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['II', 'CI', 'AI', 'CC', 'CA', 'AC', 'AA', 'F', 'L']\n",
      "Initialy declared actions: ['NA']\n",
      "Transition from II with no action and targets ['CI', 'AI'] with weights [2, 1]\n",
      "Transition from CI with no action and targets ['CC'] with weights [1]\n",
      "Transition from CC with no action and targets ['F', 'L'] with weights [1, 1]\n",
      "Transition from CA with no action and targets ['F'] with weights [1]\n",
      "Transition from AC with no action and targets ['F'] with weights [1]\n",
      "Transition from AA with no action and targets ['L'] with weights [1]\n",
      "Transition from AI with no action and targets ['AA'] with weights [1]\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - State II reward wasn't assigned, using zero as reward\n",
      "( 1 ) - State CI reward wasn't assigned, using zero as reward\n",
      "( 2 ) - State AI reward wasn't assigned, using zero as reward\n",
      "( 3 ) - State CC reward wasn't assigned, using zero as reward\n",
      "( 4 ) - State CA reward wasn't assigned, using zero as reward\n",
      "( 5 ) - State AC reward wasn't assigned, using zero as reward\n",
      "( 6 ) - State AA reward wasn't assigned, using zero as reward\n",
      "( 7 ) - State F reward wasn't assigned, using zero as reward\n",
      "( 8 ) - State L reward wasn't assigned, using zero as reward\n"
     ]
    }
   ],
   "source": [
    "mc_c2p41 = run_mdp(path = \"mdp_examples//exemple_cours_c2p41_mc.mdp\", return_printer=True, print_transactions=False) # Exemple sans récompenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- S segments:\n",
      "S_sure=['F', 'AC', 'CA']\n",
      "S_may=['II', 'CI', 'CC']\n",
      "S_never=['L', 'AI', 'AA']\n",
      "--------- System constraints:\n",
      "Ubfollower=[]\n",
      "A_ub=None\n",
      "b_ub=None\n",
      "A_eq=array([[ 1.        , -0.66666667, -0.        ],\n",
      "       [-0.        ,  1.        , -1.        ],\n",
      "       [-0.        , -0.        ,  1.        ]])\n",
      "b_eq=[array([0.]), array([0.]), array([0.5])]\n",
      "c=array([1., 1., 1.])\n",
      "--------- Solution:\n",
      "res.x=array([0.33333333, 0.5       , 0.5       ])\n",
      "res.fun=1.3333333333333333\n",
      "res.success=True\n",
      "res.slack=array([], dtype=float64)\n",
      "res.con=array([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "abc_analysis(mc_c2p41, 'F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing for a simple MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['I', 'W', 'L']\n",
      "Initialy declared actions: ['a', 'b', 'c']\n",
      "Transition from I with action a and targets ['W', 'L'] with weights [1, 1]\n",
      "Transition from I with action b and targets ['W'] with weights [1]\n",
      "Transition from I with action c and targets ['L'] with weights [1]\n",
      "Transition from W with no action and targets ['W'] with weights [1]\n",
      "Transition from L with no action and targets ['L'] with weights [1]\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - State I reward wasn't assigned, using zero as reward\n",
      "( 1 ) - State W reward wasn't assigned, using zero as reward\n",
      "( 2 ) - State L reward wasn't assigned, using zero as reward\n"
     ]
    }
   ],
   "source": [
    "simple_mdp = run_mdp(path = \"mdp_examples//simple_mdp.mdp\", return_printer=True, print_transactions=False) # Exemple sans récompenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- S segments:\n",
      "S_sure=['W']\n",
      "S_may=['I']\n",
      "S_never=['L']\n",
      "--------- System constraints:\n",
      "Ubfollower=['I', ['a', 'b', 'c']]\n",
      "A_ub=array([[-1.],\n",
      "       [-1.],\n",
      "       [-1.]])\n",
      "b_ub=[-0.5, -1.0, -0.0]\n",
      "A_eq=None\n",
      "b_eq=None\n",
      "c=array([1.])\n",
      "--------- Solution:\n",
      "res.x=array([1.])\n",
      "res.fun=1.0\n",
      "res.success=True\n",
      "res.slack=array([0.5, 0. , 1. ])\n",
      "res.con=array([], dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "abc_analysis(simple_mdp, 'W')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing for another MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['II', 'CI', 'AI', 'CC', 'CA', 'AC', 'AA', 'F', 'L']\n",
      "Initialy declared actions: ['NA', 'a', 'p', 'c']\n",
      "Transition from II with action c and targets ['CI'] with weights [1]\n",
      "Transition from II with action p and targets ['CI', 'AI'] with weights [2, 1]\n",
      "Transition from II with action a and targets ['AI'] with weights [1]\n",
      "Transition from CI with action a and targets ['CA'] with weights [1]\n",
      "Transition from CI with action c and targets ['CC'] with weights [1]\n",
      "Transition from CI with action p and targets ['CC', 'CA'] with weights [2, 1]\n",
      "Transition from CC with no action and targets ['L', 'F'] with weights [1, 1]\n",
      "Transition from CA with no action and targets ['F'] with weights [1]\n",
      "Transition from F with no action and targets ['F'] with weights [1]\n",
      "Transition from L with no action and targets ['L'] with weights [1]\n",
      "Transition from AI with action c and targets ['AC'] with weights [1]\n",
      "Transition from AI with action a and targets ['AA'] with weights [1]\n",
      "Transition from AI with action p and targets ['AC', 'AA'] with weights [2, 1]\n",
      "Transition from AA with no action and targets ['L'] with weights [1]\n",
      "Transition from AC with no action and targets ['F'] with weights [1]\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - State II reward wasn't assigned, using zero as reward\n",
      "( 1 ) - State CI reward wasn't assigned, using zero as reward\n",
      "( 2 ) - State AI reward wasn't assigned, using zero as reward\n",
      "( 3 ) - State CC reward wasn't assigned, using zero as reward\n",
      "( 4 ) - State CA reward wasn't assigned, using zero as reward\n",
      "( 5 ) - State AC reward wasn't assigned, using zero as reward\n",
      "( 6 ) - State AA reward wasn't assigned, using zero as reward\n",
      "( 7 ) - State F reward wasn't assigned, using zero as reward\n",
      "( 8 ) - State L reward wasn't assigned, using zero as reward\n"
     ]
    }
   ],
   "source": [
    "mdp_c2p40 = run_mdp(path = \"mdp_examples//exemple_cours_c2p40_mdp.mdp\", return_printer=True, print_transactions=False) # Exemple sans récompenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- S segments:\n",
      "S_sure=['F', 'AC', 'CA']\n",
      "S_may=['II', 'AI', 'CI', 'CC']\n",
      "S_never=['L', 'AA']\n",
      "--------- System constraints:\n",
      "Ubfollower=['II', ['c', 'p', 'a'], 'AI', ['c', 'a', 'p'], 'CI', ['a', 'c', 'p']]\n",
      "A_ub=array([[-1.        ,  0.        ,  1.        ,  0.        ],\n",
      "       [-1.        ,  0.33333333,  0.66666667,  0.        ],\n",
      "       [-1.        ,  1.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -1.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -1.        ,  0.        ,  0.        ],\n",
      "       [ 0.        , -1.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        , -1.        ,  0.        ],\n",
      "       [ 0.        ,  0.        , -1.        ,  1.        ],\n",
      "       [ 0.        ,  0.        , -1.        ,  0.66666667]])\n",
      "b_ub=[-0.0, -0.0, -0.0, -1.0, -0.0, -0.6666666666666666, -1.0, -0.0, -0.3333333333333333]\n",
      "A_eq=array([[-0., -0., -0.,  1.]])\n",
      "b_eq=[array([0.5])]\n",
      "c=array([1., 1., 1., 1.])\n",
      "--------- Solution:\n",
      "res.x=array([1. , 1. , 1. , 0.5])\n",
      "res.fun=3.5\n",
      "res.success=True\n",
      "res.slack=array([-0.        , -0.        ,  0.        ,  0.        ,  1.        ,\n",
      "        0.33333333,  0.        ,  0.5       ,  0.33333333])\n",
      "res.con=array([0.])\n"
     ]
    }
   ],
   "source": [
    "abc_analysis(mdp_c2p40, 'F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 14\n",
    "\n",
    "Expliquer comment adapter les algorithmes de vérification de l'exercice 13 pour le calcul de la récompense attendue pour des modèles de récompense Markoviens (MC et MDP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul de l'Espérance de Récompense pour les MC et MDP\n",
    "\n",
    "Pour calculer l'espérance de récompense dans des modèles Markoviens, tels que les chaînes de Markov (MC) et les processus de décision de Markov (MDP), nous adoptons une approche basée sur les équations et inéquations qui intègrent les probabilités de transition entre états et les récompenses associées à ces transitions.\n",
    "\n",
    "#### Chaînes de Markov (MC)\n",
    "\n",
    "Pour une chaîne de Markov, l'espérance de récompense d'un état donné, notée $X_{\\text{état}}$, est calculée via l'équation suivante :\n",
    "\n",
    "$$X_{\\text{état}} = \\text{reward}_{\\text{état}} + \\sum_{\\text{tous états}} (\\text{probabilité de transition vers cet état} \\times X_{\\text{autres états}})$$\n",
    "\n",
    "Ici, $X_{\\text{état}}$ représente l'espérance de récompense pour l'état concerné, $\\text{reward}_{\\text{état}}$ est la récompense immédiate reçue en étant dans cet état, et la somme calcule l'espérance de récompense pondérée par les probabilités de transition vers tous les autres états.\n",
    "\n",
    "#### Processus de Décision de Markov (MDP)\n",
    "\n",
    "Dans le contexte des MDP, où les décisions (actions) influencent les transitions, l'espérance de récompense pour chaque action spécifique dans chaque état est définie par l'inéquation :\n",
    "\n",
    "$$X_{\\text{état}}^{\\text{action}} \\leq \\text{reward}_{\\text{état}} + \\sum_{\\text{tous états}} (\\text{probabilité de transition vers cet état en utilisant l'action} \\times X_{\\text{autres états}})$$\n",
    "\n",
    "$X_{\\text{état}}^{\\text{action}}$ désigne ici l'espérance de récompense pour un état donné lors de l'exécution d'une action spécifique. Cette formulation sous forme d'inéquation reflète le principe selon lequel, pour un MDP, l'espérance de récompense de choisir une action spécifique est contrainte par la somme de la récompense immédiate et de l'espérance de récompense des états suivants, cette dernière étant pondérée par les probabilités de transition utilisant l'action choisie.\n",
    "\n",
    "L'emploi d'inéquations pour les MDP illustre le processus de choix et d'optimisation des actions : l'objectif est de déterminer la politique (c.-à-d., la sélection des actions dans chaque état) qui maximise l'espérance globale de récompense. Cela implique de maximiser la valeur de $X_{\\text{état}}$ pour chaque état par le biais d'une sélection optimale d'actions.\n",
    "\n",
    "Cette méthode offre un cadre pour modéliser et calculer l'espérance de récompense dans les MC et MDP, prenant en compte la structure décisionnelle et les transitions probabilistes entre états, et vise à optimiser les décisions en maximisant la récompense attendue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 15\n",
    "Simulateur de lancers de pièces successifs pour simuler un dé à 6 faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.13.1\n",
      "Initialy declared states: ['I', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6']\n",
      "Initialy declared actions: ['a']\n",
      "Transition from I with no action and targets ['T1', 'T2'] with weights [1, 1]\n",
      "Transition from T1 with no action and targets ['T3', 'T4'] with weights [1, 1]\n",
      "Transition from T2 with no action and targets ['T5', 'T6'] with weights [1, 1]\n",
      "Transition from T3 with no action and targets ['S1', 'T1'] with weights [1, 1]\n",
      "Transition from T4 with no action and targets ['S2', 'S3'] with weights [1, 1]\n",
      "Transition from T5 with no action and targets ['S4', 'S5'] with weights [1, 1]\n",
      "Transition from T6 with no action and targets ['S6', 'T2'] with weights [1, 1]\n",
      "Transition from S1 with no action and targets ['S1'] with weights [1]\n",
      "Transition from S2 with no action and targets ['S2'] with weights [1]\n",
      "Transition from S3 with no action and targets ['S3'] with weights [1]\n",
      "Transition from S4 with no action and targets ['S4'] with weights [1]\n",
      "Transition from S5 with no action and targets ['S5'] with weights [1]\n",
      "Transition from S6 with no action and targets ['S6'] with weights [1]\n",
      "\n",
      " ------- transactions df -------\n",
      "  Origin Action    I   T1   T2   T3   T4   T5   T6   S1   S2   S3   S4   S5  \\\n",
      "0      I     NA  NaN    1    1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "1     T1     NA  NaN  NaN  NaN    1    1  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "2     T2     NA  NaN  NaN  NaN  NaN  NaN    1    1  NaN  NaN  NaN  NaN  NaN   \n",
      "3     T3     NA  NaN    1  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN  NaN  NaN   \n",
      "4     T4     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1    1  NaN  NaN   \n",
      "5     T5     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1    1   \n",
      "6     T6     NA  NaN  NaN    1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "7     S1     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN  NaN  NaN   \n",
      "8     S2     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN  NaN   \n",
      "9     S3     NA  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN    1  NaN  NaN   \n",
      "\n",
      "    S6  \n",
      "0  NaN  \n",
      "1  NaN  \n",
      "2  NaN  \n",
      "3  NaN  \n",
      "4  NaN  \n",
      "5  NaN  \n",
      "6    1  \n",
      "7  NaN  \n",
      "8  NaN  \n",
      "9  NaN  \n",
      "\n",
      " ------- transactions_prob df -------\n",
      "  Origin Action    I   T1   T2   T3   T4   T5   T6   S1   S2   S3   S4   S5  \\\n",
      "0      I     NA  0.0  0.5  0.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "1     T1     NA  0.0  0.0  0.0  0.5  0.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2     T2     NA  0.0  0.0  0.0  0.0  0.0  0.5  0.5  0.0  0.0  0.0  0.0  0.0   \n",
      "3     T3     NA  0.0  0.5  0.0  0.0  0.0  0.0  0.0  0.5  0.0  0.0  0.0  0.0   \n",
      "4     T4     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.5  0.5  0.0  0.0   \n",
      "5     T5     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.5  0.5   \n",
      "6     T6     NA  0.0  0.0  0.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "7     S1     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
      "8     S2     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
      "9     S3     NA  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
      "\n",
      "    S6  \n",
      "0  0.0  \n",
      "1  0.0  \n",
      "2  0.0  \n",
      "3  0.0  \n",
      "4  0.0  \n",
      "5  0.0  \n",
      "6  0.5  \n",
      "7  0.0  \n",
      "8  0.0  \n",
      "9  0.0   \n",
      "\n",
      "\n",
      " ---------- WARNINGS WHEN PARSING -----------\n",
      "( 0 ) - State I reward wasn't assigned, using zero as reward\n",
      "( 1 ) - State T1 reward wasn't assigned, using zero as reward\n",
      "( 2 ) - State T2 reward wasn't assigned, using zero as reward\n",
      "( 3 ) - State T3 reward wasn't assigned, using zero as reward\n",
      "( 4 ) - State T4 reward wasn't assigned, using zero as reward\n",
      "( 5 ) - State T5 reward wasn't assigned, using zero as reward\n",
      "( 6 ) - State T6 reward wasn't assigned, using zero as reward\n",
      "( 7 ) - State S1 reward wasn't assigned, using zero as reward\n",
      "( 8 ) - State S2 reward wasn't assigned, using zero as reward\n",
      "( 9 ) - State S3 reward wasn't assigned, using zero as reward\n",
      "( 10 ) - State S4 reward wasn't assigned, using zero as reward\n",
      "( 11 ) - State S5 reward wasn't assigned, using zero as reward\n",
      "( 12 ) - State S6 reward wasn't assigned, using zero as reward\n"
     ]
    }
   ],
   "source": [
    "de_6_faces = run_mdp(path = \"prof_examples//simu-mc.mdp\", return_printer=True, print_transactions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img_de_6_faces.png \"MC qui simule un dé à 6 faces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction ci-dessous correspond à un générateur d'adversaires qui propose par défaut un choix aléatoire d'actions. Cette fonction nous servira lorsque nous mettrons en œuvre l'algorithme\n",
    "qui traite des processus de décision markoviens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def gerar_preferencias_acoes(df, estados, acoes, modo=\"random\"):\n",
    "    \"\"\"\n",
    "    Generates a dictionary mapping each state to a list of preferred actions.\n",
    "\n",
    "    This function allows for the specification of action preferences for each state\n",
    "    within a Markov Decision Process (MDP), either through direct user input or\n",
    "    by generating a random order of actions.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): A DataFrame containing the transition probabilities for each state-action pair.\n",
    "    - estados (list): A list of all states declared in the MDP.\n",
    "    - acoes (list): A list of all actions declared in the MDP.\n",
    "    - modo (str): The mode of preference generation, either 'input' for user-defined preferences or 'random' for automatically generated preferences.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are states and values are lists of actions, ordered by preference.\n",
    "    \"\"\"\n",
    "    preferencias = {}\n",
    "\n",
    "    for estado in estados:\n",
    "        acoes_possiveis = df[df['Origin'] == estado]['Action'].unique()\n",
    "        \n",
    "        # Filtrar ações possíveis que não sejam 'NA', caso existam\n",
    "        acoes_validas = [acao for acao in acoes_possiveis if acao != \"NA\"]\n",
    "        \n",
    "        if modo == \"input\":\n",
    "            # Se houver apenas uma ação válida ou a ação é 'NA', seleção automática\n",
    "            if len(acoes_validas) <= 1:\n",
    "                preferencias[estado] = acoes_validas if acoes_validas else [\"NA\"]\n",
    "            else:\n",
    "                print(f\"\\nCurrent State: {estado}\")\n",
    "                print(\"Possible Actions: \" + \", \".join(acoes_validas))\n",
    "                preferred = input(f\"Type in the preferred action for {estado} \\n\").strip()\n",
    "                while preferred not in acoes_validas:\n",
    "                    preferred = input(f\"he inputed action {preferred} isn't available for this state, choose one in {acoes_validas} \\n\").strip()\n",
    "                preferencias[estado] = [preferred]\n",
    "\n",
    "        elif modo == \"random\":\n",
    "            random.shuffle(acoes_validas)\n",
    "            preferencias[estado] = acoes_validas\n",
    "            \n",
    "    return preferencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonteCarlo_random_walk(p, target_state, num_transitions = 20):\n",
    "    \"\"\"\n",
    "    Simulates a random walk through an MDP based on action preferences and transition probabilities.\n",
    "\n",
    "    This function simulates traversing through a Markov Decision Process, making decisions at each state\n",
    "    based on predefined or user-specified action preferences. It illustrates the potential path an agent\n",
    "    might take, considering the MDP's transition probabilities for each action.\n",
    "\n",
    "    Parameters:\n",
    "    - p: An object containing the MDP structure, including its states, actions, transition probabilities, and other relevant data.\n",
    "\n",
    "    Returns:\n",
    "    - None: This function does not return a value but prints the simulation results, including the traversed path, actions taken, probabilities of transitions, and the total path probability.\n",
    "    \"\"\"\n",
    "        \n",
    "    df = p.transactions_prob\n",
    "    found = False\n",
    "\n",
    "    preferencias = gerar_preferencias_acoes(df, p.declared_states,p.declared_actions, modo=\"random\")\n",
    "\n",
    "    estado_atual = p.first_state\n",
    "\n",
    "    caminho = [estado_atual]  # Iniciar o registro do caminho com o estado inicial\n",
    "    probabilidade_acumulada = 1\n",
    "\n",
    "    for _ in range(num_transitions):\n",
    "        df_estado_atual = df[df['Origin'] == estado_atual]\n",
    "        if df_estado_atual.empty:\n",
    "            print(\"Stopped at a end of graph state\")\n",
    "            return\n",
    "        acao_selecionada = None\n",
    "        probabilidade_escolhida = None\n",
    "\n",
    "        if df_estado_atual.iloc[0]['Action'] == \"NA\":\n",
    "            probabilidades = df_estado_atual.iloc[0, 2:].astype(float).values\n",
    "            acao_selecionada = \"NA\"\n",
    "        else:\n",
    "            for acao_preferida in preferencias[estado_atual]:\n",
    "                df_acao_preferida = df_estado_atual[df_estado_atual['Action'] == acao_preferida]\n",
    "                if not df_acao_preferida.empty:\n",
    "                    probabilidades = df_acao_preferida.iloc[0, 2:].astype(float).values\n",
    "                    acao_selecionada = acao_preferida\n",
    "                    break\n",
    "\n",
    "        probabilidades = probabilidades / np.sum(probabilidades)\n",
    "        estados_possiveis = df_estado_atual.columns[2:]\n",
    "        proximo_estado = np.random.choice(estados_possiveis, p=probabilidades)\n",
    "        probabilidade_escolhida = probabilidades[np.where(estados_possiveis == proximo_estado)[0][0]]\n",
    "        \n",
    "        probabilidade_acumulada *= probabilidade_escolhida\n",
    "        estado_passado = estado_atual\n",
    "        estado_atual = proximo_estado\n",
    "        caminho.append(estado_atual)  # Atualizar o caminho\n",
    "\n",
    "        if target_state in caminho:\n",
    "            found = True\n",
    "            break\n",
    "\n",
    "\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target state was not reached.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "if MonteCarlo_random_walk(de_6_faces, target_state=\"S1\"):\n",
    "    print(\"The target state was reached.\")\n",
    "else:\n",
    "    print(\"The target state was not reached.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 16\n",
    "Algorithme de SMC quantitatif - simulant un dé à 6 faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonteCarloSimulator(p, target_state, num_simulations = 1000, num_transitions = 20):\n",
    "    \"\"\"\n",
    "    Simulates a random walk through an MDP based on action preferences and transition probabilities.\n",
    "\n",
    "    This function simulates traversing through a Markov Decision Process, making decisions at each state\n",
    "    based on predefined or user-specified action preferences. It illustrates the potential path an agent\n",
    "    might take, considering the MDP's transition probabilities for each action.\n",
    "\n",
    "    Parameters:\n",
    "    - p: An object containing the MDP structure, including its states, actions, transition probabilities, and other relevant data.\n",
    "\n",
    "    Returns:\n",
    "    - None: This function does not return a value but prints the simulation results, including the traversed path, actions taken, probabilities of transitions, and the total path probability.\n",
    "    \"\"\"\n",
    "    found = 0\n",
    "    for i in range(num_simulations):\n",
    "        if MonteCarlo_random_walk(p, target_state, num_transitions):\n",
    "            found += 1\n",
    "    prob_reacing = round(found/num_simulations,4)\n",
    "    print(f\"Target state {target_state} was found in {found} out of {num_simulations} simulations. Probability of reaching it: {prob_reacing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target state S1 was found in 1644 out of 10000 simulations. Probability of reaching it: 0.1644\n"
     ]
    }
   ],
   "source": [
    "MonteCarloSimulator(de_6_faces, target_state=\"S1\", num_simulations=100, num_transitions=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 17.1\n",
    "\n",
    "Implémenter l’algorithme de SMC qualitatif utilisant SPRT (Sequential Probability Ratio Test [Wald, 1945]) dans le programme de l’exercice 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SPRT(p, target_state, p0, p1, alpha, beta, num_transitions=20):\n",
    "    \"\"\"\n",
    "    Sequential Probability Ratio Test (SPRT) to determine if the probability of reaching a target state\n",
    "    exceeds a specific threshold, with predefined significance levels and power.\n",
    "    \n",
    "    Parameters:\n",
    "    - p: MDP or MC structure.\n",
    "    - target_state: The target state we want to test for.\n",
    "    - p0: The probability of success under the null hypothesis H0.\n",
    "    - p1: The probability of success under the alternative hypothesis H1.\n",
    "    - alpha: The significance level (probability of rejecting H0 when H0 is true).\n",
    "    - beta: The test power (1 - probability of accepting H0 when H1 is true).\n",
    "    - num_transitions: Maximum number of transitions per simulation.\n",
    "    \n",
    "    Returns:\n",
    "    - A string indicating the test outcome (accept H0, accept H1, or continue data collection).\n",
    "    \"\"\"\n",
    "    log_lambda = 0  # Log likelihood ratio\n",
    "    A = np.log((1 - beta) / alpha)  # Threshold to accept H1\n",
    "    B = np.log(beta / (1 - alpha))  # Threshold to accept H0\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        result = MonteCarlo_random_walk(p, target_state, num_transitions)\n",
    "        \n",
    "        # Calculate the log likelihood ratio based on the result\n",
    "        if result:\n",
    "            log_likelihood_ratio = np.log(p1 / p0)\n",
    "        else:\n",
    "            log_likelihood_ratio = np.log((1 - p1) / (1 - p0))\n",
    "        \n",
    "        log_lambda += log_likelihood_ratio\n",
    "\n",
    "        if log_lambda >= A:\n",
    "            return \"Accept H1: The probability of reaching the target state is significantly high.\"\n",
    "        elif log_lambda <= B:\n",
    "            return \"Accept H0: The probability of reaching the target state is not significantly high.\"\n",
    "        # Otherwise, continue collecting data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ##### \"Accepter H1 : La probabilité d'atteindre l'état cible est significativement élevée.\"\n",
    "\n",
    "Signification : Le test a déterminé, avec le niveau de confiance spécifié par $1 - \\alpha$, que la probabilité d'atteindre l'état cible est significativement plus élevée que la probabilité sous l'hypothèse nulle $p_0$. Cela signifie que, sur la base des simulations effectuées, il existe des preuves suffisantes pour rejeter l'hypothèse nulle en faveur de l'hypothèse alternative. En d'autres termes, il est très probable que la véritable probabilité d'atteindre l'état cible soit supérieure à $p_0$, jusqu'au seuil défini par $p_1$.\n",
    "\n",
    "- #### \"Accepter H0 : La probabilité d'atteindre l'état cible n'est pas significativement élevée.\"\n",
    "\n",
    "Signification : Le test a conclu, avec le niveau de confiance spécifié par $\\beta$, qu'il n'y a pas de preuves suffisantes pour affirmer que la probabilité d'atteindre l'état cible est supérieure à la probabilité sous l'hypothèse alternative $p_1$. Par conséquent, l'hypothèse nulle n'est pas rejetée. Cela ne signifie pas nécessairement que l'hypothèse nulle est vraie, mais que, sur la base des simulations réalisées, il n'a pas été possible de démontrer que la probabilité de succès est significativement élevée comme défini par $p_1$. On peut dire que, dans le niveau de signification défini, la probabilité d'atteindre l'état cible ressemble davantage à $p_0$ ou est inférieure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accept H1: The probability of reaching the target state is significantly high.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPRT(de_6_faces, target_state=\"S1\", p0=0.1, p1=0.15, alpha=0.01, beta=0.01, num_transitions=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 17.2\n",
    "On appelle $p_{ki}$ la probabilité d’obtenir le chiffre $i$ après $k$ lancers au maximum. Utiliser votre programme pour estimer $P(p_{ki} \\geq 0.1)$, $P(p_{ki} \\geq 0.14)$, $P(p_{ki} \\geq 0.16)$. Qu’observez-vous ? Utiliser $\\alpha = \\beta = 0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def estima_probabilites(p, target_state, seuils, alpha, beta, num_transitions=20, num_simulations=1000):\n",
    "    \"\"\"\n",
    "    Estime les probabilités P(p_{ki} >= seuil) pour chaque seuil spécifié.\n",
    "    \n",
    "    Paramètres :\n",
    "    - p : La structure MDP ou MC.\n",
    "    - target_state : L'état cible à tester.\n",
    "    - seuils : Une liste des seuils (p0) à tester.\n",
    "    - alpha : Le niveau de signification pour rejeter H0.\n",
    "    - beta : La puissance du test (1 - probabilité d'accepter H0 quand H1 est vrai).\n",
    "    - num_transitions : Nombre maximum de transitions par simulation.\n",
    "    - num_simulations : Nombre de simulations à exécuter pour chaque seuil.\n",
    "    \n",
    "    Retourne :\n",
    "    - Un dictionnaire avec les seuils comme clés et les probabilités estimées comme valeurs.\n",
    "    \"\"\"\n",
    "    resultats = {seuil: 0 for seuil in seuils}\n",
    "    p1 = max(seuils) + 0.05  # Définir p1 comme une valeur un peu plus élevée que le seuil le plus élevé\n",
    "\n",
    "    for seuil in seuils:\n",
    "        accepte_h1 = 0\n",
    "        \n",
    "        for _ in tqdm(range(num_simulations), desc=f\"Seuil {seuil}\"):\n",
    "            decision = SPRT(p, target_state, seuil, p1, alpha, beta, num_transitions)\n",
    "            if decision == \"Accept H1: The probability of reaching the target state is significantly high.\":\n",
    "                accepte_h1 += 1\n",
    "        \n",
    "        probabilite_estimee = accepte_h1 / num_simulations\n",
    "        resultats[seuil] = probabilite_estimee\n",
    "        \n",
    "    return resultats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seuil 0.1:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seuil 0.1: 100%|██████████| 10/10 [00:20<00:00,  2.02s/it]\n",
      "Seuil 0.14: 100%|██████████| 10/10 [00:47<00:00,  4.73s/it]\n",
      "Seuil 0.16: 100%|██████████| 10/10 [00:42<00:00,  4.28s/it]\n",
      "Seuil 0.17: 100%|██████████| 10/10 [01:01<00:00,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.1: 0.8, 0.14: 0.1, 0.16: 0.0, 0.17: 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "p = de_6_faces\n",
    "target_state = \"S1\"\n",
    "seuils = [0.1, 0.14, 0.16, 0.17]\n",
    "alpha = 0.01\n",
    "beta = 0.01\n",
    "num_transitions = 10\n",
    "num_simulations = 10\n",
    "\n",
    "resultats = estima_probabilites(p, target_state, seuils, alpha, beta, num_transitions, num_simulations)\n",
    "print(resultats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 17.3\n",
    "Tester sur le modèle du CRAPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 17.4\n",
    "Que faire si le modèle est un MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre code est suffisamment robuste pour traiter un PDM, puisque l'utilisation de Monte Carlo RandomWalk présuppose déjà le choix d'un adversaire aléatoire.\n",
    "\n",
    "Il nous suffit d'instancier un MDP et d'appeler la fonction en lui passant un argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modélisation Probabiliste et Apprentissage par Renforcement (Chapitre 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_10_infoia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
